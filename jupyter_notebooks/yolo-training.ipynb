{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 12479683,
     "sourceType": "datasetVersion",
     "datasetId": 7874253
    },
    {
     "sourceId": 12479684,
     "sourceType": "datasetVersion",
     "datasetId": 7874254
    }
   ],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install ultralytics pycocotools -q",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:47:58.939322Z",
     "iopub.execute_input": "2025-07-16T12:47:58.939557Z",
     "iopub.status.idle": "2025-07-16T12:48:02.098431Z",
     "shell.execute_reply.started": "2025-07-16T12:47:58.939535Z",
     "shell.execute_reply": "2025-07-16T12:48:02.097592Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as TQDM\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "from ultralytics.utils.files import increment_path\n",
    "from ultralytics.data.converter import coco91_to_coco80_class"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:02.099582Z",
     "iopub.execute_input": "2025-07-16T12:48:02.100231Z",
     "iopub.status.idle": "2025-07-16T12:48:05.742523Z",
     "shell.execute_reply.started": "2025-07-16T12:48:02.100174Z",
     "shell.execute_reply": "2025-07-16T12:48:05.741930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 640\n",
    "PROJECT_NAME = \"yolo8n_pt_640_coco_full\"\n",
    "BASE_MODEL = \"yolov8n.pt\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PATIENCE = 20\n",
    "\n",
    "INPUT_DATASET_ROOT = \"/kaggle/input/coco-sama-train/coco_sama\"\n",
    "DATASET_ROOT = \"/kaggle/working/dataset\"\n",
    "TRAINING_ROOT = \"/kaggle/working/training\"\n",
    "\n",
    "PATHS = {\n",
    "    'input_root': INPUT_DATASET_ROOT,\n",
    "    'dataset_root': DATASET_ROOT,\n",
    "    'training_root': TRAINING_ROOT,\n",
    "    'train_images': os.path.join(INPUT_DATASET_ROOT, \"train\"),\n",
    "    'val_images': os.path.join(INPUT_DATASET_ROOT, \"val\"),\n",
    "    'train_annotations': os.path.join(INPUT_DATASET_ROOT, \"annotations\", \"instances_train.json\"),\n",
    "    'val_annotations': os.path.join(INPUT_DATASET_ROOT, \"annotations\", \"instances_val.json\"),\n",
    "    'yaml_file': os.path.join(DATASET_ROOT, \"yolo_main.yaml\"),\n",
    "    'trained_model': os.path.join(TRAINING_ROOT, PROJECT_NAME, \"weights\", \"best.pt\"),\n",
    "}\n",
    "\n",
    "print(\"The configuration is set:\")\n",
    "print(f\"  - EPOCHS: {EPOCHS}\")\n",
    "print(f\"  - BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"  - IMAGE_SIZE: {IMAGE_SIZE}\")\n",
    "print(f\"  - DEVICE: {DEVICE}\")\n",
    "print(f\"  - BASE_MODEL: {BASE_MODEL}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:05.743902Z",
     "iopub.execute_input": "2025-07-16T12:48:05.744231Z",
     "iopub.status.idle": "2025-07-16T12:48:05.819628Z",
     "shell.execute_reply.started": "2025-07-16T12:48:05.744213Z",
     "shell.execute_reply": "2025-07-16T12:48:05.818720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "The configuration is set:\n  - EPOCHS: 50\n  - BATCH_SIZE: 128\n  - IMAGE_SIZE: 512\n  - DEVICE: cuda\n  - BASE_MODEL: yolov8n.pt\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "random.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nultralytics.checks()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:05.820495Z",
     "iopub.execute_input": "2025-07-16T12:48:05.820723Z",
     "iopub.status.idle": "2025-07-16T12:48:05.864838Z",
     "shell.execute_reply.started": "2025-07-16T12:48:05.820696Z",
     "shell.execute_reply": "2025-07-16T12:48:05.864329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Ultralytics 8.3.167 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete ✅ (4 CPUs, 31.4 GB RAM, 6411.3/8062.4 GB disk)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_convert_coco(\n",
    "    labels_dir: str = \"../coco/annotations/\",\n",
    "    save_dir: str = \"coco_converted/\",\n",
    "    cls91to80: bool = True,\n",
    "    bbox_area_threshold_min: float = 0.0001,\n",
    "    bbox_area_threshold_max: float = 0.8,\n",
    "    use_symlinks: bool = False,\n",
    "    skip_crowd_images: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert COCO-style JSON annotations into YOLO format, filtering by bbox area\n",
    "    and optionally dropping images marked with iscrowd==1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_dir : str\n",
    "        Path to COCO 'annotations' folder.\n",
    "    save_dir : str\n",
    "        Output root for 'images' and 'labels' subfolders.\n",
    "    cls91to80 : bool\n",
    "        Map COCO91 IDs to COCO80 if True.\n",
    "    bbox_area_threshold_min : float\n",
    "        Min relative bbox area (w*h / img_area) to keep an annotation.\n",
    "    bbox_area_threshold_max : float\n",
    "        Max relative bbox area to keep an annotation; if exceeded, drop image.\n",
    "    use_symlinks : bool\n",
    "        Create symlinks to the original images instead of copying.\n",
    "    skip_crowd_images : bool\n",
    "        If True, drop any image that has at least one annotation with iscrowd==1.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir).expanduser().resolve()\n",
    "    (save_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "    (save_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    coco80 = coco91_to_coco80_class()\n",
    "    all_categories = {}\n",
    "\n",
    "    n_images_saved = 0\n",
    "    n_images_skipped_big_bbox = 0\n",
    "    n_images_skipped_crowd = 0\n",
    "    n_annotations_removed_small = 0\n",
    "    n_annotations_kept = 0\n",
    "    kept_train, kept_val = [], []\n",
    "\n",
    "    for json_file in sorted(Path(labels_dir).glob(\"*.json\")):\n",
    "        split = \"train\" if \"train\" in json_file.stem.lower() else \"val\"\n",
    "        data = json.loads(json_file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # build category lookup\n",
    "        for cat in data.get(\"categories\", []):\n",
    "            all_categories[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "        # index images and annotations by image_id\n",
    "        images = {str(img['id']): img for img in data['images']}\n",
    "        anns_by_image = defaultdict(list)\n",
    "        crowd_by_image = defaultdict(bool)\n",
    "        for ann in data['annotations']:\n",
    "            img_id = str(ann['image_id'])\n",
    "            anns_by_image[img_id].append(ann)\n",
    "            if ann.get('iscrowd', 0) == 1:\n",
    "                crowd_by_image[img_id] = True\n",
    "\n",
    "        # process each image\n",
    "        for img_id, img in TQDM(images.items(), desc=f\"Processing {json_file.name}\"):\n",
    "            # skip if crowd flag and we want to drop crowd images\n",
    "            if skip_crowd_images and crowd_by_image.get(img_id, False):\n",
    "                n_images_skipped_crowd += 1\n",
    "                continue\n",
    "\n",
    "            w, h = img['width'], img['height']\n",
    "            img_area = w * h\n",
    "            fname = img['file_name']\n",
    "            anns = anns_by_image.get(img_id, [])\n",
    "\n",
    "            valid_anns = []\n",
    "            skip_image = False\n",
    "\n",
    "            # filter annotations by relative bbox area\n",
    "            for ann in anns:\n",
    "                bw, bh = ann['bbox'][2], ann['bbox'][3]\n",
    "                rel = (bw * bh) / img_area\n",
    "                if rel < bbox_area_threshold_min:\n",
    "                    n_annotations_removed_small += 1\n",
    "                    continue\n",
    "                if rel > bbox_area_threshold_max:\n",
    "                    skip_image = True\n",
    "                    break\n",
    "                valid_anns.append(ann)\n",
    "\n",
    "            if skip_image:\n",
    "                n_images_skipped_big_bbox += 1\n",
    "                continue\n",
    "\n",
    "            # record kept filename\n",
    "            (kept_train if split == \"train\" else kept_val).append(fname)\n",
    "\n",
    "            # prepare output directories\n",
    "            img_dir = save_dir / \"images\" / split\n",
    "            lbl_dir = save_dir / \"labels\" / split\n",
    "            img_dir.mkdir(parents=True, exist_ok=True)\n",
    "            lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # copy or symlink the image\n",
    "            src = Path(labels_dir).parent / split / fname\n",
    "            dst = img_dir / fname\n",
    "            if use_symlinks:\n",
    "                if not dst.exists():\n",
    "                    os.symlink(src, dst)\n",
    "            else:\n",
    "                if src.exists():\n",
    "                    shutil.copy(src, dst)\n",
    "                else:\n",
    "                    print(f\"Image not found: {src}, skipping.\")\n",
    "\n",
    "            # write YOLO-format label file\n",
    "            out_f = lbl_dir / Path(fname).with_suffix(\".txt\")\n",
    "            with open(out_f, \"w\", encoding=\"utf-8\") as out:\n",
    "                for ann in valid_anns:\n",
    "                    box = np.array(ann['bbox'], dtype=float)\n",
    "                    # convert from [x, y, w, h] to [x_center, y_center, w, h] normalized\n",
    "                    box[:2] += box[2:] / 2\n",
    "                    box[[0, 2]] /= w\n",
    "                    box[[1, 3]] /= h\n",
    "                    cls = (\n",
    "                        coco80[ann['category_id'] - 1]\n",
    "                        if cls91to80\n",
    "                        else ann['category_id'] - 1\n",
    "                    )\n",
    "                    line = [cls] + box.tolist()\n",
    "                    out.write((\"%g \" * len(line)).rstrip() % tuple(line) + \"\\n\")\n",
    "\n",
    "            n_images_saved += 1\n",
    "            n_annotations_kept += len(valid_anns)\n",
    "\n",
    "    # report stats\n",
    "    print(f\"Saved images: {n_images_saved}\")\n",
    "    print(f\"Skipped (large bbox): {n_images_skipped_big_bbox}\")\n",
    "    print(f\"Skipped (crowd images): {n_images_skipped_crowd}\")\n",
    "    print(f\"Removed small anns: {n_annotations_removed_small}\")\n",
    "    print(f\"Kept anns: {n_annotations_kept}\")\n",
    "    print(f\"Train count: {len(kept_train)}, Val count: {len(kept_val)}\")\n",
    "\n",
    "    yaml_cats = {i - 1: n for i, n in all_categories.items()}\n",
    "    return (\n",
    "        save_dir,\n",
    "        kept_train,\n",
    "        kept_val,\n",
    "        yaml_cats,\n",
    "        n_images_saved,\n",
    "        n_images_skipped_big_bbox,\n",
    "        n_images_skipped_crowd,\n",
    "        n_annotations_removed_small,\n",
    "        n_annotations_kept,\n",
    "    )\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:05.865530Z",
     "iopub.execute_input": "2025-07-16T12:48:05.865767Z",
     "iopub.status.idle": "2025-07-16T12:48:13.059131Z",
     "shell.execute_reply.started": "2025-07-16T12:48:05.865743Z",
     "shell.execute_reply": "2025-07-16T12:48:13.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Annotations /kaggle/input/coco-train/coco/annotations/instances_train.json: 100%|██████████| 19500/19500 [00:02<00:00, 6872.11it/s]\nAnnotations /kaggle/input/coco-train/coco/annotations/instances_val.json: 100%|██████████| 5789/5789 [00:00<00:00, 6883.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "COCO data converted successfully.\nResults saved to /kaggle/working/dataset\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_dir, kept_tr, kept_val, yaml_cats, *rest = custom_convert_coco(\n",
    "    labels_dir=os.path.join(PATHS['input_root'], \"annotations\"),\n",
    "    save_dir=PATHS['dataset_root'],\n",
    "    bbox_area_threshold_min=0.0005,\n",
    "    bbox_area_threshold_max=0.7,\n",
    "    use_symlinks=True,\n",
    "    skip_crowd_images=True\n",
    ")\n",
    "\n",
    "# Тепер:\n",
    "# - `kept_tr` та `skip_tr` — списки файлів train\n",
    "# - `kept_val` та `skip_val` — списки файлів val\n",
    "# - `all_cats` — {оригінальний id: назва}\n",
    "# - `yaml_cats` — {id-1: назва} для запису в YAML"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Creating a YAML configuration for YOLO\n",
    "data_yaml = {\n",
    "    \"path\": PATHS['dataset_root'],\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\": \"images/val\",\n",
    "    \"names\": yaml_cats\n",
    "}\n",
    "\n",
    "with open(PATHS['yaml_file'], \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"YAML configuration is saved: {PATHS['yaml_file']}\")\n",
    "print(\"YAML configuration:\")\n",
    "print(yaml.dump(data_yaml, default_flow_style=False, allow_unicode=True))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Model initialization: {BASE_MODEL}\")\n",
    "model = YOLO(BASE_MODEL)\n",
    "print(f\"Model initialized. Device in use: {DEVICE}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Image size: {IMAGE_SIZE}\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Patience: {PATIENCE}\")\n",
    "\n",
    "training_results = model.train(\n",
    "    data=PATHS['yaml_file'],\n",
    "    epochs=EPOCHS,\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    project=PATHS['training_root'],\n",
    "    name=PROJECT_NAME,\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "\n",
    "trained_model_path = PATHS['trained_model']\n",
    "print(f\"Model saved: {trained_model_path}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_training_readme():\n",
    "    \"\"\"Create a comprehensive README for the training results\"\"\"\n",
    "\n",
    "    # Get training results path\n",
    "    results_path = os.path.join(PATHS['training_root'], PROJECT_NAME)\n",
    "    readme_path = os.path.join(results_path, 'README.md')\n",
    "\n",
    "    # Read results if available\n",
    "    results_csv = os.path.join(results_path, 'results.csv')\n",
    "\n",
    "    readme_content = f\"\"\"# YOLO Training Results - {PROJECT_NAME}\n",
    "## Training Configuration\n",
    "**Model:** {BASE_MODEL}\n",
    "**Project:** {PROJECT_NAME}\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "### Hyperparameters\n",
    "- **Epochs:** {EPOCHS}\n",
    "- **Batch Size:** {BATCH_SIZE}\n",
    "- **Image Size:** {IMAGE_SIZE}\n",
    "- **Device:** {DEVICE}\n",
    "- **Patience:** {PATIENCE}\n",
    "- **Seed:** {SEED}\n",
    "### Dataset Information\n",
    "- **Dataset Source:** `{INPUT_DATASET_ROOT}`\n",
    "- **Training Images:** {len(kept_tr)}\n",
    "- **Validation Images:** {len(kept_val)}\n",
    "- **Total Images:** {len(kept_tr) + len(kept_val)}\n",
    "### Class Configuration\n",
    "**Class Names:**\n",
    "```yaml\n",
    "{yaml.dump(yaml_cats, default_flow_style=False, allow_unicode=True)}```\n",
    "## Training Results\n",
    "\"\"\"\n",
    "\n",
    "    # Add training metrics if results.csv exists\n",
    "    if os.path.exists(results_csv):\n",
    "        try:\n",
    "            df = pd.read_csv(results_csv)\n",
    "            last_epoch = df.iloc[-1]\n",
    "\n",
    "            readme_content += f\"\"\"### Final Metrics (Epoch {int(last_epoch['epoch'])})\n",
    "- **Train Box Loss:** {last_epoch.get('train/box_loss', 'N/A'):.4f}\n",
    "- **Train Class Loss:** {last_epoch.get('train/cls_loss', 'N/A'):.4f}\n",
    "- **Train DFL Loss:** {last_epoch.get('train/dfl_loss', 'N/A'):.4f}\n",
    "- **Validation mAP50:** {last_epoch.get('metrics/mAP50(B)', 'N/A'):.4f}\n",
    "- **Validation mAP50-95:** {last_epoch.get('metrics/mAP50-95(B)', 'N/A'):.4f}\n",
    "- **Validation Precision:** {last_epoch.get('metrics/precision(B)', 'N/A'):.4f}\n",
    "- **Validation Recall:** {last_epoch.get('metrics/recall(B)', 'N/A'):.4f}\n",
    "### Training Progress\n",
    "\"\"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            readme_content += f\"*Error loading training results: {str(e)}*\\n\\n\"\n",
    "\n",
    "    # Add file structure and environment info\n",
    "    readme_content += f\"\"\"## Project Structure\n",
    "\n",
    "## Training Environment\n",
    "- **Python Version:** 3.11.11\n",
    "- **PyTorch Version:** {torch.__version__}\n",
    "- **Ultralytics Version:** {ultralytics.__version__}\n",
    "- **CUDA Available:** {torch.cuda.is_available()}\n",
    "---\n",
    "*Generated automatically on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "\n",
    "    # Write README\n",
    "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "\n",
    "    print(f\"✅ README created: {readme_path}\")\n",
    "\n",
    "    # Also create a summary JSON\n",
    "    summary_data = {\n",
    "        \"project_name\": PROJECT_NAME,\n",
    "        \"model\": BASE_MODEL,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "        \"device\": DEVICE,\n",
    "        \"train_images\": len(kept_tr),\n",
    "        \"val_images\": len(kept_val),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    summary_path = os.path.join(results_path, 'training_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Summary JSON created: {summary_path}\")\n",
    "    return readme_path\n",
    "\n",
    "\n",
    "create_training_readme()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "zip_filename = f\"{PROJECT_NAME}_training.zip\"\n",
    "folder_to_zip = \"training\"\n",
    "\n",
    "print(f\"📦 Creating archive: {zip_filename}\")\n",
    "\n",
    "!zip -r -q {zip_filename} {folder_to_zip}/\n",
    "\n",
    "zip_path = f'/kaggle/working/{zip_filename}'\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    file_size = os.path.getsize(zip_path) / (1024*1024)  # у MB\n",
    "    print(f\"\\n✅ Archive created successfully!\")\n",
    "    print(f\"📁 File: {zip_filename}\")\n",
    "    print(f\"📊 Size: {file_size:.1f} MB\")\n",
    "    print(f\"📍 Path: {zip_path}\")\n",
    "\n",
    "    print(f\"\\n📋 Archive contents:\")\n",
    "    !zipinfo {zip_filename} | head -20\n",
    "\n",
    "    display(HTML(f'''\n",
    "    <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "        <h3>📥 Download Ready</h3>\n",
    "        <a href=\"{zip_filename}\" download style=\"background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;\">\n",
    "            📥 Download {zip_filename} ({file_size:.1f} MB)\n",
    "        </a>\n",
    "    </div>\n",
    "    '''))\n",
    "else:\n",
    "    print(\"❌ Error: Archive not created!\")\n",
    "    print(\"📁 Checking working directory contents:\")\n",
    "    !ls -la /kaggle/working/"
   ]
  }
 ]
}
