{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 12479683,
     "sourceType": "datasetVersion",
     "datasetId": 7874253
    },
    {
     "sourceId": 12479684,
     "sourceType": "datasetVersion",
     "datasetId": 7874254
    }
   ],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install ultralytics pycocotools -q",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:47:58.939322Z",
     "iopub.execute_input": "2025-07-16T12:47:58.939557Z",
     "iopub.status.idle": "2025-07-16T12:48:02.098431Z",
     "shell.execute_reply.started": "2025-07-16T12:47:58.939535Z",
     "shell.execute_reply": "2025-07-16T12:48:02.097592Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as TQDM\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "from ultralytics.utils.files import increment_path\n",
    "from ultralytics.data.converter import coco91_to_coco80_class"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:02.099582Z",
     "iopub.execute_input": "2025-07-16T12:48:02.100231Z",
     "iopub.status.idle": "2025-07-16T12:48:05.742523Z",
     "shell.execute_reply.started": "2025-07-16T12:48:02.100174Z",
     "shell.execute_reply": "2025-07-16T12:48:05.741930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 512\n",
    "PROJECT_NAME = \"yolo8n_pt_512_coco_skiped_crowd_3_85\"\n",
    "BASE_MODEL = \"yolov8n.pt\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PATIENCE = 20\n",
    "\n",
    "INPUT_DATASET_ROOT = \"/kaggle/input/coco-train/coco\"\n",
    "DATASET_ROOT = \"/kaggle/working/dataset\"\n",
    "TRAINING_ROOT = \"/kaggle/working/training\"\n",
    "\n",
    "PATHS = {\n",
    "    'input_root': INPUT_DATASET_ROOT,\n",
    "    'dataset_root': DATASET_ROOT,\n",
    "    'training_root': TRAINING_ROOT,\n",
    "    'train_images': os.path.join(INPUT_DATASET_ROOT, \"train\"),\n",
    "    'val_images': os.path.join(INPUT_DATASET_ROOT, \"val\"),\n",
    "    'train_annotations': os.path.join(INPUT_DATASET_ROOT, \"annotations\", \"instances_train.json\"),\n",
    "    'val_annotations': os.path.join(INPUT_DATASET_ROOT, \"annotations\", \"instances_val.json\"),\n",
    "    'yaml_file': os.path.join(DATASET_ROOT, \"yolo_main.yaml\"),\n",
    "    'trained_model': os.path.join(TRAINING_ROOT, PROJECT_NAME, \"weights\", \"best.pt\"),\n",
    "}\n",
    "\n",
    "BBOX_AREA_TRESHOLD_MIN = 0.003\n",
    "BBOX_AREA_TRESHOLD_MAX = 0.85\n",
    "USE_SIMLINKS = True\n",
    "SKIP_CROWD_IMAGES = True\n",
    "\n",
    "print(\"The configuration is set:\")\n",
    "print(f\"  - EPOCHS: {EPOCHS}\")\n",
    "print(f\"  - BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"  - IMAGE_SIZE: {IMAGE_SIZE}\")\n",
    "print(f\"  - DEVICE: {DEVICE}\")\n",
    "print(f\"  - BASE_MODEL: {BASE_MODEL}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:05.743902Z",
     "iopub.execute_input": "2025-07-16T12:48:05.744231Z",
     "iopub.status.idle": "2025-07-16T12:48:05.819628Z",
     "shell.execute_reply.started": "2025-07-16T12:48:05.744213Z",
     "shell.execute_reply": "2025-07-16T12:48:05.818720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "The configuration is set:\n  - EPOCHS: 50\n  - BATCH_SIZE: 128\n  - IMAGE_SIZE: 512\n  - DEVICE: cuda\n  - BASE_MODEL: yolov8n.pt\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "random.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nultralytics.checks()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:05.820495Z",
     "iopub.execute_input": "2025-07-16T12:48:05.820723Z",
     "iopub.status.idle": "2025-07-16T12:48:05.864838Z",
     "shell.execute_reply.started": "2025-07-16T12:48:05.820696Z",
     "shell.execute_reply": "2025-07-16T12:48:05.864329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Ultralytics 8.3.167 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete ‚úÖ (4 CPUs, 31.4 GB RAM, 6411.3/8062.4 GB disk)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_convert_coco(\n",
    "    labels_dir: str = \"../coco/annotations/\",\n",
    "    save_dir: str = \"coco_converted/\",\n",
    "    cls91to80: bool = True,\n",
    "    bbox_area_threshold_min: float = 0,\n",
    "    bbox_area_threshold_max: float = 1,\n",
    "    use_symlinks: bool = False,\n",
    "    skip_crowd_images: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert COCO-style JSON annotations into YOLO format, filtering by bbox area\n",
    "    and optionally dropping images marked with iscrowd==1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_dir : str\n",
    "        Path to COCO 'annotations' folder.\n",
    "    save_dir : str\n",
    "        Output root for 'images' and 'labels' subfolders.\n",
    "    cls91to80 : bool\n",
    "        Map COCO91 IDs to COCO80 if True.\n",
    "    bbox_area_threshold_min : float\n",
    "        Min relative bbox area (w*h / img_area) to keep an annotation.\n",
    "    bbox_area_threshold_max : float\n",
    "        Max relative bbox area to keep an annotation; if exceeded, drop image.\n",
    "    use_symlinks : bool\n",
    "        Create symlinks to the original images instead of copying.\n",
    "    skip_crowd_images : bool\n",
    "        If True, drop any image that has at least one annotation with iscrowd==1.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir).expanduser().resolve()\n",
    "    (save_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "    (save_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    coco80 = coco91_to_coco80_class()\n",
    "    all_categories = {}\n",
    "\n",
    "    n_images_saved = 0\n",
    "    n_images_skipped_big_bbox = 0\n",
    "    n_images_skipped_crowd = 0\n",
    "    n_annotations_removed_small = 0\n",
    "    n_annotations_kept = 0\n",
    "    kept_train, kept_val = [], []\n",
    "\n",
    "    for json_file in sorted(Path(labels_dir).glob(\"*.json\")):\n",
    "        split = \"train\" if \"train\" in json_file.stem.lower() else \"val\"\n",
    "        data = json.loads(json_file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # build category lookup\n",
    "        for cat in data.get(\"categories\", []):\n",
    "            all_categories[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "        # index images and annotations by image_id\n",
    "        images = {str(img['id']): img for img in data['images']}\n",
    "        anns_by_image = defaultdict(list)\n",
    "        crowd_by_image = defaultdict(bool)\n",
    "        for ann in data['annotations']:\n",
    "            img_id = str(ann['image_id'])\n",
    "            anns_by_image[img_id].append(ann)\n",
    "            if ann.get('iscrowd', 0) == 1:\n",
    "                crowd_by_image[img_id] = True\n",
    "\n",
    "        # process each image\n",
    "        for img_id, img in TQDM(images.items(), desc=f\"Processing {json_file.name}\"):\n",
    "            # skip if crowd flag and we want to drop crowd images\n",
    "            if skip_crowd_images and crowd_by_image.get(img_id, False):\n",
    "                n_images_skipped_crowd += 1\n",
    "                continue\n",
    "\n",
    "            w, h = img['width'], img['height']\n",
    "            img_area = w * h\n",
    "            fname = img['file_name']\n",
    "            anns = anns_by_image.get(img_id, [])\n",
    "\n",
    "            valid_anns = []\n",
    "            skip_image = False\n",
    "\n",
    "            # filter annotations by relative bbox area\n",
    "            for ann in anns:\n",
    "                bw, bh = ann['bbox'][2], ann['bbox'][3]\n",
    "                rel = (bw * bh) / img_area\n",
    "                if rel < bbox_area_threshold_min:\n",
    "                    n_annotations_removed_small += 1\n",
    "                    continue\n",
    "                if rel > bbox_area_threshold_max:\n",
    "                    skip_image = True\n",
    "                    break\n",
    "                valid_anns.append(ann)\n",
    "\n",
    "            if skip_image:\n",
    "                n_images_skipped_big_bbox += 1\n",
    "                continue\n",
    "\n",
    "            # record kept filename\n",
    "            (kept_train if split == \"train\" else kept_val).append(fname)\n",
    "\n",
    "            # prepare output directories\n",
    "            img_dir = save_dir / \"images\" / split\n",
    "            lbl_dir = save_dir / \"labels\" / split\n",
    "            img_dir.mkdir(parents=True, exist_ok=True)\n",
    "            lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # copy or symlink the image\n",
    "            src = Path(labels_dir).parent / split / fname\n",
    "            dst = img_dir / fname\n",
    "            if use_symlinks:\n",
    "                if not dst.exists():\n",
    "                    os.symlink(src, dst)\n",
    "            else:\n",
    "                if src.exists():\n",
    "                    shutil.copy(src, dst)\n",
    "                else:\n",
    "                    print(f\"Image not found: {src}, skipping.\")\n",
    "\n",
    "            # write YOLO-format label file\n",
    "            out_f = lbl_dir / Path(fname).with_suffix(\".txt\")\n",
    "            with open(out_f, \"w\", encoding=\"utf-8\") as out:\n",
    "                for ann in valid_anns:\n",
    "                    box = np.array(ann['bbox'], dtype=float)\n",
    "                    # convert from [x, y, w, h] to [x_center, y_center, w, h] normalized\n",
    "                    box[:2] += box[2:] / 2\n",
    "                    box[[0, 2]] /= w\n",
    "                    box[[1, 3]] /= h\n",
    "                    cls = (\n",
    "                        coco80[ann['category_id'] - 1]\n",
    "                        if cls91to80\n",
    "                        else ann['category_id'] - 1\n",
    "                    )\n",
    "                    line = [cls] + box.tolist()\n",
    "                    out.write((\"%g \" * len(line)).rstrip() % tuple(line) + \"\\n\")\n",
    "\n",
    "            n_images_saved += 1\n",
    "            n_annotations_kept += len(valid_anns)\n",
    "\n",
    "    # report stats\n",
    "    print(f\"Saved images: {n_images_saved}\")\n",
    "    print(f\"Skipped (large bbox): {n_images_skipped_big_bbox}\")\n",
    "    print(f\"Skipped (crowd images): {n_images_skipped_crowd}\")\n",
    "    print(f\"Removed small anns: {n_annotations_removed_small}\")\n",
    "    print(f\"Kept anns: {n_annotations_kept}\")\n",
    "    print(f\"Train count: {len(kept_train)}, Val count: {len(kept_val)}\")\n",
    "\n",
    "    yaml_cats = {i - 1: n for i, n in all_categories.items()}\n",
    "    return (\n",
    "        save_dir,\n",
    "        kept_train,\n",
    "        kept_val,\n",
    "        yaml_cats,\n",
    "        n_images_saved,\n",
    "        n_images_skipped_big_bbox,\n",
    "        n_images_skipped_crowd,\n",
    "        n_annotations_removed_small,\n",
    "        n_annotations_kept,\n",
    "    )\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-16T12:48:05.865530Z",
     "iopub.execute_input": "2025-07-16T12:48:05.865767Z",
     "iopub.status.idle": "2025-07-16T12:48:13.059131Z",
     "shell.execute_reply.started": "2025-07-16T12:48:05.865743Z",
     "shell.execute_reply": "2025-07-16T12:48:13.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Annotations /kaggle/input/coco-train/coco/annotations/instances_train.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19500/19500 [00:02<00:00, 6872.11it/s]\nAnnotations /kaggle/input/coco-train/coco/annotations/instances_val.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5789/5789 [00:00<00:00, 6883.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "COCO data converted successfully.\nResults saved to /kaggle/working/dataset\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_dir, kept_tr, kept_val, yaml_cats, n_images_saved, n_images_skipped_big_bbox, n_images_skipped_crowd, n_annotations_removed_small, n_annotations_kept = custom_convert_coco(\n",
    "    labels_dir=os.path.join(PATHS['input_root'], \"annotations\"),\n",
    "    save_dir=PATHS['dataset_root'],\n",
    "    bbox_area_threshold_min=BBOX_AREA_TRESHOLD_MIN,\n",
    "    bbox_area_threshold_max=BBOX_AREA_TRESHOLD_MAX,\n",
    "    use_symlinks=USE_SIMLINKS,\n",
    "    skip_crowd_images=SKIP_CROWD_IMAGES\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Creating a YAML configuration for YOLO\n",
    "data_yaml = {\n",
    "    \"path\": PATHS['dataset_root'],\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\": \"images/val\",\n",
    "    \"names\": yaml_cats\n",
    "}\n",
    "\n",
    "with open(PATHS['yaml_file'], \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"YAML configuration is saved: {PATHS['yaml_file']}\")\n",
    "print(\"YAML configuration:\")\n",
    "print(yaml.dump(data_yaml, default_flow_style=False, allow_unicode=True))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Model initialization: {BASE_MODEL}\")\n",
    "model = YOLO(BASE_MODEL)\n",
    "print(f\"Model initialized. Device in use: {DEVICE}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Image size: {IMAGE_SIZE}\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Patience: {PATIENCE}\")\n",
    "\n",
    "training_results = model.train(\n",
    "    data=PATHS['yaml_file'],\n",
    "    epochs=EPOCHS,\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    project=PATHS['training_root'],\n",
    "    name=PROJECT_NAME,\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "\n",
    "trained_model_path = PATHS['trained_model']\n",
    "print(f\"Model saved: {trained_model_path}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_training_readme():\n",
    "    \"\"\"Create a comprehensive README for the training results with detailed information\"\"\"\n",
    "\n",
    "    # Get training results path\n",
    "    results_path = os.path.join(PATHS['training_root'], PROJECT_NAME)\n",
    "    readme_path = os.path.join(results_path, 'README.md')\n",
    "\n",
    "    # Read results if available\n",
    "    results_csv = os.path.join(results_path, 'results.csv')\n",
    "\n",
    "    # Analyze dataset for detailed class statistics\n",
    "    def analyze_dataset_classes():\n",
    "        \"\"\"Analyze the converted dataset to get detailed class statistics\"\"\"\n",
    "        class_stats = defaultdict(lambda: {'train': 0, 'val': 0, 'total': 0})\n",
    "        train_labels_dir = os.path.join(PATHS['dataset_root'], 'labels', 'train')\n",
    "        val_labels_dir = os.path.join(PATHS['dataset_root'], 'labels', 'val')\n",
    "\n",
    "        total_train_annotations = 0\n",
    "        total_val_annotations = 0\n",
    "\n",
    "        # Analyze training labels\n",
    "        if os.path.exists(train_labels_dir):\n",
    "            for label_file in Path(train_labels_dir).glob('*.txt'):\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if line.strip():\n",
    "                            class_id = int(line.strip().split()[0])\n",
    "                            class_stats[class_id]['train'] += 1\n",
    "                            total_train_annotations += 1\n",
    "\n",
    "        # Analyze validation labels\n",
    "        if os.path.exists(val_labels_dir):\n",
    "            for label_file in Path(val_labels_dir).glob('*.txt'):\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if line.strip():\n",
    "                            class_id = int(line.strip().split()[0])\n",
    "                            class_stats[class_id]['val'] += 1\n",
    "                            total_val_annotations += 1\n",
    "\n",
    "        # Calculate totals\n",
    "        for class_id in class_stats:\n",
    "            class_stats[class_id]['total'] = class_stats[class_id]['train'] + class_stats[class_id]['val']\n",
    "\n",
    "        return dict(class_stats), total_train_annotations, total_val_annotations\n",
    "\n",
    "    # Get detailed class statistics\n",
    "    class_stats, total_train_anns, total_val_anns = analyze_dataset_classes()\n",
    "\n",
    "    # Calculate additional dataset metrics\n",
    "    train_val_ratio = len(kept_tr) / (len(kept_tr) + len(kept_val)) if (len(kept_tr) + len(kept_val)) > 0 else 0\n",
    "    avg_anns_per_train_img = total_train_anns / len(kept_tr) if len(kept_tr) > 0 else 0\n",
    "    avg_anns_per_val_img = total_val_anns / len(kept_val) if len(kept_val) > 0 else 0\n",
    "\n",
    "    readme_content = f\"\"\"# YOLO Training Results - {PROJECT_NAME}\n",
    "\n",
    "## üìã Training Overview\n",
    "**Model:** {BASE_MODEL}\n",
    "**Project:** {PROJECT_NAME}\n",
    "**Training Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Seed:** {SEED} (for reproducibility)\n",
    "**Training Duration:** {'Completed' if os.path.exists(results_csv) else 'In Progress'}\n",
    "\n",
    "## ‚öôÔ∏è Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| **Epochs** | {EPOCHS} |\n",
    "| **Batch Size** | {BATCH_SIZE} |\n",
    "| **Image Size** | {IMAGE_SIZE}√ó{IMAGE_SIZE} |\n",
    "| **Device** | {DEVICE} |\n",
    "| **Patience** | {PATIENCE} |\n",
    "| **Random Seed** | {SEED} |\n",
    "| **Learning Rate** | Default YOLO |\n",
    "| **Optimizer** | AdamW |\n",
    "| **Weight Decay** | Default YOLO |\n",
    "\n",
    "### Environment Information\n",
    "| Component | Version/Info |\n",
    "|-----------|--------------|\n",
    "| **Python** | 3.11.11 |\n",
    "| **PyTorch** | {torch.__version__} |\n",
    "| **Ultralytics** | {ultralytics.__version__} |\n",
    "| **CUDA Available** | {torch.cuda.is_available()} |\n",
    "| **GPU Memory** | {'Available' if torch.cuda.is_available() else 'N/A'} |\n",
    "| **Device Used** | {DEVICE} |\n",
    "| **Platform** | Kaggle Notebook |\n",
    "\n",
    "## üìä Dataset Information\n",
    "\n",
    "### Dataset Overview\n",
    "| Metric | Value | Percentage |\n",
    "|--------|-------|------------|\n",
    "| **Dataset Source** | `{INPUT_DATASET_ROOT}` | - |\n",
    "| **Training Images** | {len(kept_tr):,} | {len(kept_tr)/(len(kept_tr) + len(kept_val))*100:.1f}% |\n",
    "| **Validation Images** | {len(kept_val):,} | {len(kept_val)/(len(kept_tr) + len(kept_val))*100:.1f}% |\n",
    "| **Total Images** | {len(kept_tr) + len(kept_val):,} | 100% |\n",
    "| **Training Annotations** | {total_train_anns:,} | {total_train_anns/(total_train_anns + total_val_anns)*100:.1f}% |\n",
    "| **Validation Annotations** | {total_val_anns:,} | {total_val_anns/(total_train_anns + total_val_anns)*100:.1f}% |\n",
    "| **Total Annotations** | {total_train_anns + total_val_anns:,} | 100% |\n",
    "\n",
    "### Dataset Quality Metrics\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Avg Annotations per Train Image** | {avg_anns_per_train_img:.2f} |\n",
    "| **Avg Annotations per Val Image** | {avg_anns_per_val_img:.2f} |\n",
    "| **Overall Avg Annotations per Image** | {(total_train_anns + total_val_anns)/(len(kept_tr) + len(kept_val)):.2f} |\n",
    "| **Dataset Balance (Train/Val)** | {train_val_ratio:.1%} / {1-train_val_ratio:.1%} |\n",
    "| **Annotation Density** | {(total_train_anns + total_val_anns)/(len(kept_tr) + len(kept_val)):.2f} objects per image |\n",
    "\n",
    "### Dataset Processing Pipeline\n",
    "| Processing Step | Count | Percentage | Notes |\n",
    "|----------------|-------|------------|-------|\n",
    "| **Original Images** | {n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd:,} | 100% | Total in source dataset |\n",
    "| **Images Saved** | {n_images_saved:,} | {n_images_saved/(n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd)*100:.1f}% | Successfully processed |\n",
    "| **Images Skipped (Large BBox)** | {n_images_skipped_big_bbox:,} | {n_images_skipped_big_bbox/(n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd)*100:.1f}% | BBox > {BBOX_AREA_TRESHOLD_MAX} of image area |\n",
    "| **Images Skipped (Crowd)** | {n_images_skipped_crowd:,} | {n_images_skipped_crowd/(n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd)*100:.1f}% | Contains crowd annotations |\n",
    "| **Annotations Removed (Small)** | {n_annotations_removed_small:,} | - | BBox < {BBOX_AREA_TRESHOLD_MIN} of image area |\n",
    "| **Annotations Kept** | {n_annotations_kept:,} | - | Final annotation count |\n",
    "\n",
    "### Data Filtering Parameters\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| **Min BBox Area Threshold** | {BBOX_AREA_TRESHOLD_MIN} | Minimum relative bbox area (w√óh / image_area) |\n",
    "| **Max BBox Area Threshold** | {BBOX_AREA_TRESHOLD_MAX} | Maximum relative bbox area (drops entire image) |\n",
    "| **Skip Crowd Images** | {SKIP_CROWD_IMAGES} | Remove images with crowd annotations |\n",
    "| **Use Symlinks** | {USE_SIMLINKS} | Create symlinks instead of copying images |\n",
    "\n",
    "## üè∑Ô∏è Class Distribution Analysis\n",
    "\n",
    "### Class Overview\n",
    "**Total Number of Classes:** {len(yaml_cats)}\n",
    "**Most Frequent Class:** {max(class_stats.keys(), key=lambda x: class_stats[x]['total']) if class_stats else 'N/A'} ({yaml_cats.get(max(class_stats.keys(), key=lambda x: class_stats[x]['total']), 'N/A') if class_stats else 'N/A'}) - {max(class_stats.values(), key=lambda x: x['total'])['total'] if class_stats else 0:,} annotations\n",
    "**Least Frequent Class:** {min(class_stats.keys(), key=lambda x: class_stats[x]['total']) if class_stats else 'N/A'} ({yaml_cats.get(min(class_stats.keys(), key=lambda x: class_stats[x]['total']), 'N/A') if class_stats else 'N/A'}) - {min(class_stats.values(), key=lambda x: x['total'])['total'] if class_stats else 0:,} annotations\"\"\"\n",
    "\n",
    "    # Add detailed class statistics table\n",
    "    if class_stats:\n",
    "        readme_content += f\"\"\"\n",
    "\n",
    "### üìä Detailed Class Statistics\n",
    "| Class ID | Class Name | Train Annotations | Val Annotations | Total Annotations | Train % | Val % | Total % |\n",
    "|----------|------------|-------------------|-----------------|-------------------|---------|-------|---------|\"\"\"\n",
    "\n",
    "        # Sort classes by total annotations (descending)\n",
    "        sorted_classes = sorted(class_stats.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "\n",
    "        for class_id, stats in sorted_classes:\n",
    "            class_name = yaml_cats.get(class_id, f'Class_{class_id}')\n",
    "            train_count = stats['train']\n",
    "            val_count = stats['val']\n",
    "            total_count = stats['total']\n",
    "            train_pct = (train_count / total_train_anns * 100) if total_train_anns > 0 else 0\n",
    "            val_pct = (val_count / total_val_anns * 100) if total_val_anns > 0 else 0\n",
    "            total_pct = (total_count / (total_train_anns + total_val_anns) * 100) if (total_train_anns + total_val_anns) > 0 else 0\n",
    "\n",
    "            readme_content += f\"\\n| {class_id} | {class_name} | {train_count:,} | {val_count:,} | {total_count:,} | {train_pct:.2f}% | {val_pct:.2f}% | {total_pct:.2f}% |\"\n",
    "\n",
    "        # Add class distribution summary\n",
    "        total_classes_with_data = len([c for c in class_stats.values() if c['total'] > 0])\n",
    "        classes_with_low_samples = len([c for c in class_stats.values() if c['total'] < 100])\n",
    "        classes_with_high_samples = len([c for c in class_stats.values() if c['total'] > 1000])\n",
    "\n",
    "        readme_content += f\"\"\"\n",
    "\n",
    "### üìà Class Distribution Summary\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Classes with Annotations** | {total_classes_with_data} / {len(yaml_cats)} |\n",
    "| **Classes with <100 Annotations** | {classes_with_low_samples} ({classes_with_low_samples/len(yaml_cats)*100:.1f}%) |\n",
    "| **Classes with >1000 Annotations** | {classes_with_high_samples} ({classes_with_high_samples/len(yaml_cats)*100:.1f}%) |\n",
    "| **Class Imbalance Ratio** | {max(class_stats.values(), key=lambda x: x['total'])['total'] / max(1, min(class_stats.values(), key=lambda x: x['total'])['total']) if class_stats else 0:.1f}:1 |\n",
    "| **Average Annotations per Class** | {(total_train_anns + total_val_anns) / len(yaml_cats):.1f} |\n",
    "| **Median Annotations per Class** | {sorted([c['total'] for c in class_stats.values()])[len(class_stats)//2] if class_stats else 0} |\"\"\"\n",
    "\n",
    "    # Add complete class mapping\n",
    "    readme_content += f\"\"\"\n",
    "\n",
    "### üóÇÔ∏è Complete Class Mapping\n",
    "```yaml\n",
    "# COCO80 Class Names (ID: Name)\n",
    "{yaml.dump(yaml_cats, default_flow_style=False, allow_unicode=True, width=float('inf'))}```\"\"\"\n",
    "\n",
    "    readme_content += f\"\"\"\n",
    "\n",
    "## üìà Training Results\"\"\"\n",
    "\n",
    "    # Add detailed training metrics if results.csv exists\n",
    "    if os.path.exists(results_csv):\n",
    "        try:\n",
    "            df = pd.read_csv(results_csv)\n",
    "            last_epoch = df.iloc[-1]\n",
    "            best_epoch_idx = df['metrics/mAP50(B)'].idxmax() if 'metrics/mAP50(B)' in df.columns else -1\n",
    "            best_epoch = df.iloc[best_epoch_idx] if best_epoch_idx != -1 else last_epoch\n",
    "\n",
    "            # Calculate training efficiency metrics\n",
    "            training_duration = len(df)\n",
    "            epochs_to_best = int(best_epoch['epoch']) + 1 if best_epoch_idx != -1 else training_duration\n",
    "            training_efficiency = (epochs_to_best / EPOCHS) * 100\n",
    "\n",
    "            readme_content += f\"\"\"\n",
    "\n",
    "### üéØ Training Summary\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Total Epochs Completed** | {training_duration} / {EPOCHS} |\n",
    "| **Best Performance at Epoch** | {int(best_epoch['epoch']) + 1} |\n",
    "| **Training Efficiency** | {training_efficiency:.1f}% (reached best at {training_efficiency:.1f}% of max epochs) |\n",
    "| **Early Stopping Triggered** | {'Yes' if training_duration < EPOCHS else 'No'} |\n",
    "| **Final Learning Rate** | {last_epoch.get('lr/pg0', 'N/A')} |\n",
    "\n",
    "### üèÜ Best Performance Metrics (Epoch {int(best_epoch['epoch']) + 1})\n",
    "| Metric | Value | Description |\n",
    "|--------|-------|-------------|\n",
    "| **mAP@0.5** | {best_epoch.get('metrics/mAP50(B)', 'N/A'):.4f} | Mean Average Precision at IoU=0.5 |\n",
    "| **mAP@0.5:0.95** | {best_epoch.get('metrics/mAP50-95(B)', 'N/A'):.4f} | Mean Average Precision averaged over IoU 0.5-0.95 |\n",
    "| **Precision** | {best_epoch.get('metrics/precision(B)', 'N/A'):.4f} | True Positives / (True Positives + False Positives) |\n",
    "| **Recall** | {best_epoch.get('metrics/recall(B)', 'N/A'):.4f} | True Positives / (True Positives + False Negatives) |\n",
    "| **F1-Score** | {2 * best_epoch.get('metrics/precision(B)', 0) * best_epoch.get('metrics/recall(B)', 0) / (best_epoch.get('metrics/precision(B)', 0) + best_epoch.get('metrics/recall(B)', 0) + 1e-16):.4f} | Harmonic mean of Precision and Recall |\n",
    "\n",
    "### üìä Final Training Metrics (Epoch {int(last_epoch['epoch']) + 1})\n",
    "\n",
    "#### Loss Metrics\n",
    "| Loss Type | Train | Validation | Description |\n",
    "|-----------|-------|------------|-------------|\n",
    "| **Box Loss** | {last_epoch.get('train/box_loss', 'N/A'):.6f} | {last_epoch.get('val/box_loss', 'N/A'):.6f} | Bounding box regression loss |\n",
    "| **Class Loss** | {last_epoch.get('train/cls_loss', 'N/A'):.6f} | {last_epoch.get('val/cls_loss', 'N/A'):.6f} | Classification loss |\n",
    "| **DFL Loss** | {last_epoch.get('train/dfl_loss', 'N/A'):.6f} | {last_epoch.get('val/dfl_loss', 'N/A'):.6f} | Distribution Focal Loss |\n",
    "\n",
    "#### Performance Metrics\n",
    "| Metric | Value | Benchmark |\n",
    "|--------|-------|-----------|\n",
    "| **Precision** | {last_epoch.get('metrics/precision(B)', 'N/A'):.4f} | >0.7 Good, >0.8 Excellent |\n",
    "| **Recall** | {last_epoch.get('metrics/recall(B)', 'N/A'):.4f} | >0.7 Good, >0.8 Excellent |\n",
    "| **mAP@0.5** | {last_epoch.get('metrics/mAP50(B)', 'N/A'):.4f} | >0.5 Good, >0.7 Excellent |\n",
    "| **mAP@0.5:0.95** | {last_epoch.get('metrics/mAP50-95(B)', 'N/A'):.4f} | >0.3 Good, >0.5 Excellent |\n",
    "| **F1-Score** | {2 * last_epoch.get('metrics/precision(B)', 0) * last_epoch.get('metrics/recall(B)', 0) / (last_epoch.get('metrics/precision(B)', 0) + last_epoch.get('metrics/recall(B)', 0) + 1e-16):.4f} | Balanced precision-recall metric |\n",
    "\n",
    "### üìä Training Progress Analysis\n",
    "| Metric | First Epoch | Last Epoch | Best Value | Improvement | Epoch of Best |\n",
    "|--------|-------------|------------|------------|-------------|---------------|\"\"\"\n",
    "\n",
    "            first_epoch = df.iloc[0]\n",
    "\n",
    "            # Calculate improvements for key metrics\n",
    "            metrics_to_track = [\n",
    "                ('mAP@0.5', 'metrics/mAP50(B)', False),\n",
    "                ('mAP@0.5:0.95', 'metrics/mAP50-95(B)', False),\n",
    "                ('Precision', 'metrics/precision(B)', False),\n",
    "                ('Recall', 'metrics/recall(B)', False),\n",
    "                ('Train Box Loss', 'train/box_loss', True),\n",
    "                ('Train Class Loss', 'train/cls_loss', True),\n",
    "                ('Val Box Loss', 'val/box_loss', True),\n",
    "                ('Val Class Loss', 'val/cls_loss', True)\n",
    "            ]\n",
    "\n",
    "            for metric_name, metric_key, is_loss in metrics_to_track:\n",
    "                if metric_key in df.columns:\n",
    "                    first_val = first_epoch.get(metric_key, 0)\n",
    "                    last_val = last_epoch.get(metric_key, 0)\n",
    "                    if is_loss:\n",
    "                        best_val = df[metric_key].min()\n",
    "                        best_epoch_num = df[metric_key].idxmin() + 1\n",
    "                        improvement = ((first_val - last_val) / first_val * 100) if first_val != 0 else 0\n",
    "                        improvement_sign = \"‚Üì\" if improvement > 0 else \"‚Üë\"\n",
    "                    else:\n",
    "                        best_val = df[metric_key].max()\n",
    "                        best_epoch_num = df[metric_key].idxmax() + 1\n",
    "                        improvement = ((last_val - first_val) / first_val * 100) if first_val != 0 else 0\n",
    "                        improvement_sign = \"‚Üë\" if improvement > 0 else \"‚Üì\"\n",
    "\n",
    "                    readme_content += f\"\\n| **{metric_name}** | {first_val:.4f} | {last_val:.4f} | {best_val:.4f} | {improvement_sign}{abs(improvement):.1f}% | {best_epoch_num} |\"\n",
    "\n",
    "            # Add learning rate and other training dynamics\n",
    "            if 'lr/pg0' in df.columns:\n",
    "                readme_content += f\"\"\"\n",
    "\n",
    "### üìâ Training Dynamics\n",
    "| Metric | Initial | Final | Description |\n",
    "|--------|---------|-------|-------------|\n",
    "| **Learning Rate (pg0)** | {df['lr/pg0'].iloc[0]:.6f} | {df['lr/pg0'].iloc[-1]:.6f} | Primary parameter group learning rate |\"\"\"\n",
    "\n",
    "                if 'lr/pg1' in df.columns:\n",
    "                    readme_content += f\"\\n| **Learning Rate (pg1)** | {df['lr/pg1'].iloc[0]:.6f} | {df['lr/pg1'].iloc[-1]:.6f} | Secondary parameter group learning rate |\"\n",
    "                if 'lr/pg2' in df.columns:\n",
    "                    readme_content += f\"\\n| **Learning Rate (pg2)** | {df['lr/pg2'].iloc[0]:.6f} | {df['lr/pg2'].iloc[-1]:.6f} | Tertiary parameter group learning rate |\"\n",
    "\n",
    "            # Add epoch-by-epoch breakdown for key metrics\n",
    "            readme_content += f\"\"\"\n",
    "\n",
    "### üìã Detailed Training Log (Key Epochs)\n",
    "| Epoch | Box Loss | Cls Loss | Val Loss | mAP@0.5 | mAP@0.5:0.95 | Precision | Recall | LR |\n",
    "|-------|----------|----------|----------|---------|--------------|-----------|--------|-----|\"\"\"\n",
    "\n",
    "            # Show strategic epochs: first, every 10th, best performance, and last\n",
    "            epochs_to_show = [0]  # First epoch\n",
    "            epochs_to_show.extend(range(9, len(df), 10))  # Every 10th epoch\n",
    "            if best_epoch_idx not in epochs_to_show and best_epoch_idx != -1:\n",
    "                epochs_to_show.append(best_epoch_idx)  # Best epoch\n",
    "            if len(df) - 1 not in epochs_to_show:\n",
    "                epochs_to_show.append(len(df) - 1)  # Last epoch\n",
    "\n",
    "            epochs_to_show = sorted(set(epochs_to_show))\n",
    "\n",
    "            for i in epochs_to_show:\n",
    "                row = df.iloc[i]\n",
    "                epoch_num = int(row['epoch']) + 1\n",
    "                box_loss = row.get('train/box_loss', 0)\n",
    "                cls_loss = row.get('train/cls_loss', 0)\n",
    "                val_loss = row.get('val/box_loss', 0) + row.get('val/cls_loss', 0)\n",
    "                map50 = row.get('metrics/mAP50(B)', 0)\n",
    "                map50_95 = row.get('metrics/mAP50-95(B)', 0)\n",
    "                precision = row.get('metrics/precision(B)', 0)\n",
    "                recall = row.get('metrics/recall(B)', 0)\n",
    "                lr = row.get('lr/pg0', 0)\n",
    "\n",
    "                # Highlight best epoch\n",
    "                marker = \" üèÜ\" if i == best_epoch_idx else \"\"\n",
    "                readme_content += f\"\\n| {epoch_num}{marker} | {box_loss:.4f} | {cls_loss:.4f} | {val_loss:.4f} | {map50:.4f} | {map50_95:.4f} | {precision:.4f} | {recall:.4f} | {lr:.6f} |\"\n",
    "\n",
    "        except Exception as e:\n",
    "            readme_content += f\"\\n\\n*‚ö†Ô∏è Error loading detailed training results: {str(e)}*\\n\"\n",
    "\n",
    "    # Add comprehensive model and deployment information\n",
    "    readme_content += f\"\"\"\n",
    "\n",
    "\n",
    "## üìä Additional Metrics & Analysis\n",
    "\n",
    "### Training Efficiency\n",
    "- **Compute Hours**: ~{EPOCHS * BATCH_SIZE * (len(kept_tr) + len(kept_val)) / (3600 * 1000):.2f} GPU hours estimated\n",
    "- **Samples per Second**: ~{BATCH_SIZE * len(kept_tr) / (EPOCHS * 60):.0f} during training\n",
    "- **Memory Usage**: ~{BATCH_SIZE * 3 * IMAGE_SIZE * IMAGE_SIZE * 4 / (1024**3):.2f} GB GPU memory (estimated)\n",
    "\n",
    "### Data Quality Assessment\n",
    "- **Annotation Quality**: Filtered for size and crowd constraints\n",
    "- **Class Balance**: {len([c for c in class_stats.values() if c['total'] > (total_train_anns + total_val_anns) / (len(yaml_cats) * 2)])} classes above average\n",
    "- **Dataset Completeness**: {len(kept_tr) + len(kept_val):,} images from {n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd:,} original\n",
    "\n",
    "\n",
    "*Generated automatically on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*Training: {len(kept_tr):,} images | Validation: {len(kept_val):,} images | Classes: {len(yaml_cats)}*\n",
    "*Device: {DEVICE} | Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE} | Image Size: {IMAGE_SIZE}√ó{IMAGE_SIZE}*\n",
    "*Total Annotations: {total_train_anns + total_val_anns:,} | Avg per Image: {(total_train_anns + total_val_anns)/(len(kept_tr) + len(kept_val)):.1f}*\n",
    "\"\"\"\n",
    "\n",
    "    # Write README\n",
    "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "\n",
    "    print(f\"‚úÖ Enhanced README created: {readme_path}\")\n",
    "\n",
    "    # Create detailed summary JSON with all available information\n",
    "    summary_data = {\n",
    "        \"project_info\": {\n",
    "            \"name\": PROJECT_NAME,\n",
    "            \"base_model\": BASE_MODEL,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"seed\": SEED,\n",
    "            \"training_completed\": os.path.exists(results_csv)\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"image_size\": IMAGE_SIZE,\n",
    "            \"device\": DEVICE,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"bbox_area_threshold_min\": BBOX_AREA_TRESHOLD_MIN,\n",
    "            \"bbox_area_threshold_max\": BBOX_AREA_TRESHOLD_MAX,\n",
    "            \"skip_crowd_images\": SKIP_CROWD_IMAGES,\n",
    "            \"use_symlinks\": USE_SIMLINKS\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"source\": INPUT_DATASET_ROOT,\n",
    "            \"train_images\": len(kept_tr),\n",
    "            \"val_images\": len(kept_val),\n",
    "            \"total_images\": len(kept_tr) + len(kept_val),\n",
    "            \"train_annotations\": total_train_anns,\n",
    "            \"val_annotations\": total_val_anns,\n",
    "            \"total_annotations\": total_train_anns + total_val_anns,\n",
    "            \"train_val_ratio\": f\"{len(kept_tr)/(len(kept_tr) + len(kept_val))*100:.1f}%/{len(kept_val)/(len(kept_tr) + len(kept_val))*100:.1f}%\",\n",
    "            \"avg_annotations_per_train_image\": avg_anns_per_train_img,\n",
    "            \"avg_annotations_per_val_image\": avg_anns_per_val_img,\n",
    "            \"avg_annotations_per_image\": (total_train_anns + total_val_anns)/(len(kept_tr) + len(kept_val)),\n",
    "            \"num_classes\": len(yaml_cats),\n",
    "            \"class_names\": yaml_cats,\n",
    "            \"class_statistics\": class_stats\n",
    "        },\n",
    "        \"data_processing\": {\n",
    "            \"images_saved\": n_images_saved,\n",
    "            \"images_skipped_big_bbox\": n_images_skipped_big_bbox,\n",
    "            \"images_skipped_crowd\": n_images_skipped_crowd,\n",
    "            \"annotations_removed_small\": n_annotations_removed_small,\n",
    "            \"annotations_kept\": n_annotations_kept,\n",
    "            \"processing_efficiency\": n_images_saved/(n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd) if (n_images_saved + n_images_skipped_big_bbox + n_images_skipped_crowd) > 0 else 0\n",
    "        },\n",
    "        \"class_analysis\": {\n",
    "            \"total_classes_with_data\": len([c for c in class_stats.values() if c['total'] > 0]) if class_stats else 0,\n",
    "            \"classes_with_low_samples\": len([c for c in class_stats.values() if c['total'] < 100]) if class_stats else 0,\n",
    "            \"classes_with_high_samples\": len([c for c in class_stats.values() if c['total'] > 1000]) if class_stats else 0,\n",
    "            \"most_frequent_class\": {\n",
    "                \"id\": max(class_stats.keys(), key=lambda x: class_stats[x]['total']) if class_stats else None,\n",
    "                \"name\": yaml_cats.get(max(class_stats.keys(), key=lambda x: class_stats[x]['total']), 'N/A') if class_stats else 'N/A',\n",
    "                \"count\": max(class_stats.values(), key=lambda x: x['total'])['total'] if class_stats else 0\n",
    "            },\n",
    "            \"least_frequent_class\": {\n",
    "                \"id\": min(class_stats.keys(), key=lambda x: class_stats[x]['total']) if class_stats else None,\n",
    "                \"name\": yaml_cats.get(min(class_stats.keys(), key=lambda x: class_stats[x]['total']), 'N/A') if class_stats else 'N/A',\n",
    "                \"count\": min(class_stats.values(), key=lambda x: x['total'])['total'] if class_stats else 0\n",
    "            },\n",
    "            \"class_imbalance_ratio\": max(class_stats.values(), key=lambda x: x['total'])['total'] / max(1, min(class_stats.values(), key=lambda x: x['total'])['total']) if class_stats else 0,\n",
    "            \"median_annotations_per_class\": sorted([c['total'] for c in class_stats.values()])[len(class_stats)//2] if class_stats else 0\n",
    "        },\n",
    "        \"environment\": {\n",
    "            \"python_version\": \"3.11.11\",\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "            \"ultralytics_version\": ultralytics.__version__,\n",
    "            \"cuda_available\": torch.cuda.is_available(),\n",
    "            \"platform\": \"Kaggle Notebook\"\n",
    "        },\n",
    "        \"paths\": {\n",
    "            \"dataset_root\": PATHS['dataset_root'],\n",
    "            \"training_root\": PATHS['training_root'],\n",
    "            \"yaml_file\": PATHS['yaml_file'],\n",
    "            \"trained_model\": PATHS['trained_model']\n",
    "        },\n",
    "        \"estimated_metrics\": {\n",
    "            \"estimated_gpu_hours\": EPOCHS * BATCH_SIZE * (len(kept_tr) + len(kept_val)) / (3600 * 1000),\n",
    "            \"estimated_samples_per_second\": BATCH_SIZE * len(kept_tr) / (EPOCHS * 60),\n",
    "            \"estimated_gpu_memory_gb\": BATCH_SIZE * 3 * IMAGE_SIZE * IMAGE_SIZE * 4 / (1024**3)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Add training results to summary if available\n",
    "    if os.path.exists(results_csv):\n",
    "        try:\n",
    "            df = pd.read_csv(results_csv)\n",
    "            last_epoch = df.iloc[-1]\n",
    "            best_epoch_idx = df['metrics/mAP50(B)'].idxmax() if 'metrics/mAP50(B)' in df.columns else -1\n",
    "            best_epoch = df.iloc[best_epoch_idx] if best_epoch_idx != -1 else last_epoch\n",
    "            first_epoch = df.iloc[0]\n",
    "\n",
    "            # Calculate training efficiency\n",
    "            training_duration = len(df)\n",
    "            epochs_to_best = int(best_epoch['epoch']) + 1 if best_epoch_idx != -1 else training_duration\n",
    "            training_efficiency = (epochs_to_best / EPOCHS) * 100\n",
    "\n",
    "            summary_data[\"training_results\"] = {\n",
    "                \"total_epochs_trained\": training_duration,\n",
    "                \"max_epochs\": EPOCHS,\n",
    "                \"epochs_to_best_performance\": epochs_to_best,\n",
    "                \"training_efficiency_percent\": training_efficiency,\n",
    "                \"early_stopping_triggered\": training_duration < EPOCHS,\n",
    "                \"final_metrics\": {\n",
    "                    \"epoch\": int(last_epoch['epoch']),\n",
    "                    \"train_box_loss\": float(last_epoch.get('train/box_loss', 0)),\n",
    "                    \"train_cls_loss\": float(last_epoch.get('train/cls_loss', 0)),\n",
    "                    \"train_dfl_loss\": float(last_epoch.get('train/dfl_loss', 0)),\n",
    "                    \"val_box_loss\": float(last_epoch.get('val/box_loss', 0)),\n",
    "                    \"val_cls_loss\": float(last_epoch.get('val/cls_loss', 0)),\n",
    "                    \"val_dfl_loss\": float(last_epoch.get('val/dfl_loss', 0)),\n",
    "                    \"precision\": float(last_epoch.get('metrics/precision(B)', 0)),\n",
    "                    \"recall\": float(last_epoch.get('metrics/recall(B)', 0)),\n",
    "                    \"map50\": float(last_epoch.get('metrics/mAP50(B)', 0)),\n",
    "                    \"map50_95\": float(last_epoch.get('metrics/mAP50-95(B)', 0)),\n",
    "                    \"f1_score\": 2 * last_epoch.get('metrics/precision(B)', 0) * last_epoch.get('metrics/recall(B)', 0) / (last_epoch.get('metrics/precision(B)', 0) + last_epoch.get('metrics/recall(B)', 0) + 1e-16),\n",
    "                    \"learning_rate\": float(last_epoch.get('lr/pg0', 0))\n",
    "                },\n",
    "                \"best_metrics\": {\n",
    "                    \"epoch\": int(best_epoch['epoch']),\n",
    "                    \"best_map50\": float(best_epoch.get('metrics/mAP50(B)', 0)),\n",
    "                    \"best_map50_95\": float(best_epoch.get('metrics/mAP50-95(B)', 0)),\n",
    "                    \"precision_at_best\": float(best_epoch.get('metrics/precision(B)', 0)),\n",
    "                    \"recall_at_best\": float(best_epoch.get('metrics/recall(B)', 0)),\n",
    "                    \"f1_score_at_best\": 2 * best_epoch.get('metrics/precision(B)', 0) * best_epoch.get('metrics/recall(B)', 0) / (best_epoch.get('metrics/precision(B)', 0) + best_epoch.get('metrics/recall(B)', 0) + 1e-16)\n",
    "                },\n",
    "                \"training_progress\": {\n",
    "                    \"initial_map50\": float(first_epoch.get('metrics/mAP50(B)', 0)),\n",
    "                    \"final_map50\": float(last_epoch.get('metrics/mAP50(B)', 0)),\n",
    "                    \"map50_improvement_percent\": ((last_epoch.get('metrics/mAP50(B)', 0) - first_epoch.get('metrics/mAP50(B)', 0)) / max(first_epoch.get('metrics/mAP50(B)', 0.001), 0.001)) * 100,\n",
    "                    \"initial_train_loss\": float(first_epoch.get('train/box_loss', 0)) + float(first_epoch.get('train/cls_loss', 0)),\n",
    "                    \"final_train_loss\": float(last_epoch.get('train/box_loss', 0)) + float(last_epoch.get('train/cls_loss', 0)),\n",
    "                    \"loss_reduction_percent\": ((first_epoch.get('train/box_loss', 0) + first_epoch.get('train/cls_loss', 0) - last_epoch.get('train/box_loss', 0) - last_epoch.get('train/cls_loss', 0)) / max(first_epoch.get('train/box_loss', 0) + first_epoch.get('train/cls_loss', 0), 0.001)) * 100\n",
    "                },\n",
    "                \"performance_benchmarks\": {\n",
    "                    \"excellent_map50_threshold\": 0.7,\n",
    "                    \"good_map50_threshold\": 0.5,\n",
    "                    \"excellent_map50_95_threshold\": 0.5,\n",
    "                    \"good_map50_95_threshold\": 0.3,\n",
    "                    \"map50_rating\": \"Excellent\" if last_epoch.get('metrics/mAP50(B)', 0) > 0.7 else \"Good\" if last_epoch.get('metrics/mAP50(B)', 0) > 0.5 else \"Needs Improvement\",\n",
    "                    \"map50_95_rating\": \"Excellent\" if last_epoch.get('metrics/mAP50-95(B)', 0) > 0.5 else \"Good\" if last_epoch.get('metrics/mAP50-95(B)', 0) > 0.3 else \"Needs Improvement\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Add detailed epoch data for key milestones\n",
    "            milestone_epochs = [0]  # First epoch\n",
    "            milestone_epochs.extend(range(9, len(df), 10))  # Every 10th epoch\n",
    "            if best_epoch_idx not in milestone_epochs and best_epoch_idx != -1:\n",
    "                milestone_epochs.append(best_epoch_idx)  # Best epoch\n",
    "            if len(df) - 1 not in milestone_epochs:\n",
    "                milestone_epochs.append(len(df) - 1)  # Last epoch\n",
    "\n",
    "            milestone_epochs = sorted(set(milestone_epochs))\n",
    "\n",
    "            summary_data[\"training_results\"][\"milestone_epochs\"] = []\n",
    "            for i in milestone_epochs:\n",
    "                row = df.iloc[i]\n",
    "                milestone_data = {\n",
    "                    \"epoch\": int(row['epoch']) + 1,\n",
    "                    \"is_best\": i == best_epoch_idx,\n",
    "                    \"is_final\": i == len(df) - 1,\n",
    "                    \"train_box_loss\": float(row.get('train/box_loss', 0)),\n",
    "                    \"train_cls_loss\": float(row.get('train/cls_loss', 0)),\n",
    "                    \"val_box_loss\": float(row.get('val/box_loss', 0)),\n",
    "                    \"val_cls_loss\": float(row.get('val/cls_loss', 0)),\n",
    "                    \"map50\": float(row.get('metrics/mAP50(B)', 0)),\n",
    "                    \"map50_95\": float(row.get('metrics/mAP50-95(B)', 0)),\n",
    "                    \"precision\": float(row.get('metrics/precision(B)', 0)),\n",
    "                    \"recall\": float(row.get('metrics/recall(B)', 0)),\n",
    "                    \"learning_rate\": float(row.get('lr/pg0', 0))\n",
    "                }\n",
    "                summary_data[\"training_results\"][\"milestone_epochs\"].append(milestone_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            summary_data[\"training_results\"] = {\"error\": str(e), \"error_type\": \"results_parsing_failed\"}\n",
    "\n",
    "    # Add comprehensive recommendations based on results\n",
    "    recommendations = []\n",
    "\n",
    "    # Dataset recommendations\n",
    "    if class_stats:\n",
    "        classes_with_low_samples = len([c for c in class_stats.values() if c['total'] < 100])\n",
    "        if classes_with_low_samples > 0:\n",
    "            recommendations.append(f\"Consider collecting more data for {classes_with_low_samples} classes with <100 annotations\")\n",
    "\n",
    "        imbalance_ratio = max(class_stats.values(), key=lambda x: x['total'])['total'] / max(1, min(class_stats.values(), key=lambda x: x['total'])['total'])\n",
    "        if imbalance_ratio > 50:\n",
    "            recommendations.append(f\"Class imbalance is high ({imbalance_ratio:.1f}:1). Consider data augmentation or weighted loss functions\")\n",
    "\n",
    "    # Training recommendations\n",
    "    if os.path.exists(results_csv):\n",
    "        try:\n",
    "            df = pd.read_csv(results_csv)\n",
    "            final_map50 = df.iloc[-1].get('metrics/mAP50(B)', 0)\n",
    "            if final_map50 < 0.5:\n",
    "                recommendations.append(\"mAP@0.5 is below 0.5. Consider increasing epochs, adjusting learning rate, or collecting more data\")\n",
    "            elif final_map50 > 0.8:\n",
    "                recommendations.append(\"Excellent mAP@0.5 performance! Model is ready for deployment\")\n",
    "\n",
    "            if len(df) < EPOCHS:\n",
    "                recommendations.append(\"Training stopped early. Model may benefit from continued training or adjusted patience\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Hardware recommendations\n",
    "    if DEVICE == 'cpu':\n",
    "        recommendations.append(\"Training on CPU detected. Consider using GPU for faster training\")\n",
    "\n",
    "    if BATCH_SIZE < 32:\n",
    "        recommendations.append(\"Small batch size detected. Consider increasing if GPU memory allows\")\n",
    "\n",
    "    summary_data[\"recommendations\"] = recommendations\n",
    "\n",
    "    # Add file structure information\n",
    "    summary_data[\"file_structure\"] = {\n",
    "        \"readme_file\": readme_path,\n",
    "        \"summary_json\": os.path.join(results_path, 'training_summary.json'),\n",
    "        \"model_weights\": PATHS['trained_model'],\n",
    "        \"dataset_yaml\": PATHS['yaml_file'],\n",
    "        \"results_csv\": results_csv if os.path.exists(results_csv) else None,\n",
    "        \"expected_visualization_files\": [\n",
    "            \"confusion_matrix.png\",\n",
    "            \"results.png\",\n",
    "            \"val_batch0_labels.jpg\",\n",
    "            \"val_batch0_pred.jpg\",\n",
    "            \"PR_curve.png\",\n",
    "            \"F1_curve.png\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    summary_path = os.path.join(results_path, 'training_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Detailed summary JSON created: {summary_path}\")\n",
    "    print(f\"üìä README includes {len(class_stats)} class statistics and comprehensive training analysis\")\n",
    "    print(f\"üéØ {len(recommendations)} recommendations generated based on training results\")\n",
    "\n",
    "    return readme_path\n",
    "\n",
    "\n",
    "create_training_readme()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "zip_filename = f\"{PROJECT_NAME}_training.zip\"\n",
    "folder_to_zip = \"training\"\n",
    "\n",
    "print(f\"üì¶ Creating archive: {zip_filename}\")\n",
    "\n",
    "!zip -r -q {zip_filename} {folder_to_zip}/\n",
    "\n",
    "zip_path = f'/kaggle/working/{zip_filename}'\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    file_size = os.path.getsize(zip_path) / (1024*1024)  # —É MB\n",
    "    print(f\"\\n‚úÖ Archive created successfully!\")\n",
    "    print(f\"üìÅ File: {zip_filename}\")\n",
    "    print(f\"üìä Size: {file_size:.1f} MB\")\n",
    "    print(f\"üìç Path: {zip_path}\")\n",
    "    \n",
    "    print(f\"\\nüìã Archive contents:\")\n",
    "    !zipinfo {zip_filename} | head -20\n",
    "    \n",
    "    display(HTML(f'''\n",
    "    <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "        <h3>üì• Download Ready</h3>\n",
    "        <a href=\"{zip_filename}\" download style=\"background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;\">\n",
    "            üì• Download {zip_filename} ({file_size:.1f} MB)\n",
    "        </a>\n",
    "    </div>\n",
    "    '''))\n",
    "else:\n",
    "    print(\"‚ùå Error: Archive not created!\")\n",
    "    print(\"üìÅ Checking working directory contents:\")\n",
    "    !ls -la /kaggle/working/"
   ]
  }
 ]
}
