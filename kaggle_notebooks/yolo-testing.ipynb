{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 12430303,
     "sourceType": "datasetVersion",
     "datasetId": 7840670
    },
    {
     "sourceId": 471580,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 380214,
     "modelId": 399975
    }
   ],
   "dockerImageVersionId": 31090,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:40.076606Z",
     "iopub.execute_input": "2025-07-15T09:33:40.076935Z",
     "iopub.status.idle": "2025-07-15T09:33:43.648320Z",
     "shell.execute_reply.started": "2025-07-15T09:33:40.076911Z",
     "shell.execute_reply": "2025-07-15T09:33:43.647697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Configuration loaded for experiment: pascal_voc_yolo_evaluation\nUsing device: cuda\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration Dictionary\n",
    "CONFIG = {\n",
    "    # Experiment Settings\n",
    "    'experiment_name': 'pascal_voc_yolo_evaluation',\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "\n",
    "    # Paths\n",
    "    'dataset_path': '/kaggle/input/pascal-voc-test/pascal_voc',\n",
    "    'annotations_path': '/kaggle/input/pascal-voc-test/pascal_voc/annotations/instances_test.json',\n",
    "    'images_path': '/kaggle/input/pascal-voc-test/pascal_voc/test',\n",
    "    'models_path': '/kaggle/input/m/yardnh/temp/pytorch/temp/1',\n",
    "    'output_base_path': '/kaggle/working/evaluation_results',\n",
    "\n",
    "    # Model Testing Settings\n",
    "    'confidence_threshold': 0.5,\n",
    "    'iou_threshold': 0.5,\n",
    "    'max_detections': 100,\n",
    "    'image_size': 512,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "    # Visualization Settings\n",
    "    'num_sample_images': 20,\n",
    "    'save_visualizations': True,\n",
    "    'show_console_output': False,\n",
    "    'plot_style': 'seaborn-v0_8',\n",
    "    'figure_size': (12, 8),\n",
    "    'dpi': 300,\n",
    "\n",
    "    # Output Settings\n",
    "    'save_coco_results': True,\n",
    "    'save_metrics': True,\n",
    "    'save_timing_info': True,\n",
    "    'generate_markdown_report': True,\n",
    "\n",
    "    # Metrics Settings\n",
    "    'calculate_map': True,\n",
    "    'map_iou_thresholds': [0.5, 0.75],\n",
    "    'calculate_per_class_metrics': True,\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded for experiment: {CONFIG['experiment_name']}\")\n",
    "print(f\"Output directory: {CONFIG['output_base_path']}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.649044Z",
     "iopub.execute_input": "2025-07-15T09:33:43.649454Z",
     "iopub.status.idle": "2025-07-15T09:33:43.655433Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.649422Z",
     "shell.execute_reply": "2025-07-15T09:33:43.654876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Directory structure created:\nâ”œâ”€â”€ evaluation_results\n    â”œâ”€â”€ visualizations\n    â”œâ”€â”€ sample_images\n    â”œâ”€â”€ predictions/ (will be created per model)\n    â”œâ”€â”€ metrics.json\n    â””â”€â”€ experiment_report.md\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create the required directory structure\"\"\"\n",
    "    base_path = Path(CONFIG['output_base_path'])\n",
    "\n",
    "    directories = [\n",
    "        base_path,\n",
    "        base_path / 'visualizations',\n",
    "        base_path / 'samples',\n",
    "        base_path / 'coco_results',\n",
    "        base_path / 'metrics',\n",
    "        base_path / 'timing'\n",
    "    ]\n",
    "\n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Directory structure created successfully\")\n",
    "    return base_path\n",
    "\n",
    "base_output_path = setup_directories()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.657030Z",
     "iopub.execute_input": "2025-07-15T09:33:43.657224Z",
     "iopub.status.idle": "2025-07-15T09:33:43.852955Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.657208Z",
     "shell.execute_reply": "2025-07-15T09:33:43.852354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loaded 6418 images\nFound 10959 annotations\nCategories: ['person', 'cat', 'dog', 'bus', 'car']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "def fix_coco_format(gt_data):\n",
    "    \"\"\"Fix COCO format by adding missing required fields\"\"\"\n",
    "    if 'info' not in gt_data:\n",
    "        gt_data['info'] = {\n",
    "            'description': 'Pascal VOC Test Dataset',\n",
    "            'url': '',\n",
    "            'version': '1.0',\n",
    "            'year': 2024,\n",
    "            'contributor': '',\n",
    "            'date_created': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    if 'licenses' not in gt_data:\n",
    "        gt_data['licenses'] = []\n",
    "\n",
    "    return gt_data"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.853725Z",
     "iopub.execute_input": "2025-07-15T09:33:43.853969Z",
     "iopub.status.idle": "2025-07-15T09:33:43.860833Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.853952Z",
     "shell.execute_reply": "2025-07-15T09:33:43.860198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 6 model files:\n  1. yolo8n_pt_512_128_orig_full.pt\n  2. yolo8n_pt_512_64_sama_full.pt\n  3. yolo8n_pt_512_16_sama_full.pt\n  4. yolo8n_yaml_512_sama_full.pt\n  5. yolo8n_pt_512_16_orig_full.pt\n  6. yolo8n_yaml_512_orig_full.pt\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "def load_ground_truth():\n",
    "    \"\"\"Load ground truth annotations in COCO format\"\"\"\n",
    "    with open(CONFIG['annotations_path'], 'r') as f:\n",
    "        gt_data = json.load(f)\n",
    "\n",
    "    # Fix COCO format if needed\n",
    "    gt_data = fix_coco_format(gt_data)\n",
    "\n",
    "    # Create category mapping - CRITICAL FIX\n",
    "    category_mapping = {}\n",
    "    yolo_to_coco = {}\n",
    "    coco_to_yolo = {}\n",
    "\n",
    "    print(\"Category mapping analysis:\")\n",
    "    for i, cat in enumerate(gt_data['categories']):\n",
    "        category_mapping[i] = cat['id']  # YOLO class index -> COCO category ID\n",
    "        yolo_to_coco[i] = cat['id']\n",
    "        coco_to_yolo[cat['id']] = i\n",
    "        print(f\"  YOLO class {i} -> COCO category {cat['id']} ({cat['name']})\")\n",
    "\n",
    "    # Write fixed data to temporary file\n",
    "    temp_gt_file = '/tmp/fixed_gt.json'\n",
    "    with open(temp_gt_file, 'w') as f:\n",
    "        json.dump(gt_data, f)\n",
    "\n",
    "    # Load with COCO API\n",
    "    coco_gt = COCO(temp_gt_file)\n",
    "\n",
    "    print(f\"\\nGround truth loaded:\")\n",
    "    print(f\"  Images: {len(gt_data['images'])}\")\n",
    "    print(f\"  Annotations: {len(gt_data['annotations'])}\")\n",
    "    print(f\"  Categories: {len(gt_data['categories'])}\")\n",
    "\n",
    "    return coco_gt, gt_data, category_mapping, yolo_to_coco, coco_to_yolo\n",
    "\n",
    "def discover_models():\n",
    "    \"\"\"Discover all YOLO models in the models directory\"\"\"\n",
    "    models_path = Path(CONFIG['models_path'])\n",
    "    model_files = list(models_path.glob('*.pt'))\n",
    "\n",
    "    print(f\"\\nFound {len(model_files)} YOLO models:\")\n",
    "    for model_file in model_files:\n",
    "        print(f\"  - {model_file.name}\")\n",
    "\n",
    "    return model_files\n",
    "\n",
    "# Load data\n",
    "coco_gt, gt_data, category_mapping, yolo_to_coco, coco_to_yolo = load_ground_truth()\n",
    "model_files = discover_models()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.861553Z",
     "iopub.execute_input": "2025-07-15T09:33:43.861828Z",
     "iopub.status.idle": "2025-07-15T09:33:43.874639Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.861805Z",
     "shell.execute_reply": "2025-07-15T09:33:43.874128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "YOLO Model wrapper ready\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "CONFIG.update({\n",
    "    'validation_sample_size': 100,\n",
    "    'show_validation_details': True,\n",
    "    'category_mapping': category_mapping,\n",
    "    'yolo_to_coco': yolo_to_coco,\n",
    "    'coco_to_yolo': coco_to_yolo\n",
    "})"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.875355Z",
     "iopub.execute_input": "2025-07-15T09:33:43.875633Z",
     "iopub.status.idle": "2025-07-15T09:33:43.893442Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.875601Z",
     "shell.execute_reply": "2025-07-15T09:33:43.892878Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "def create_image_id_mapping(coco_gt, images_path):\n",
    "    \"\"\"Create mapping between image filenames and COCO image IDs\"\"\"\n",
    "    image_files = list(Path(images_path).glob('*.jpg')) + list(Path(images_path).glob('*.png'))\n",
    "\n",
    "    # Get image info from COCO ground truth\n",
    "    coco_images = {img['file_name']: img['id'] for img in coco_gt.dataset['images']}\n",
    "\n",
    "    # Create mapping for available images\n",
    "    filename_to_id = {}\n",
    "    id_to_filename = {}\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_filename = img_file.name\n",
    "\n",
    "        # Try to find matching image in COCO dataset\n",
    "        if img_filename in coco_images:\n",
    "            img_id = coco_images[img_filename]\n",
    "        else:\n",
    "            # Try without extension or with different extension\n",
    "            stem = img_file.stem\n",
    "            matched_id = None\n",
    "            for coco_filename, coco_id in coco_images.items():\n",
    "                if Path(coco_filename).stem == stem:\n",
    "                    matched_id = coco_id\n",
    "                    break\n",
    "\n",
    "            if matched_id is not None:\n",
    "                img_id = matched_id\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        filename_to_id[img_filename] = img_id\n",
    "        id_to_filename[img_id] = img_filename\n",
    "\n",
    "    print(f\"Image mapping: {len(filename_to_id)} images matched to COCO IDs\")\n",
    "    return filename_to_id, id_to_filename\n",
    "\n",
    "def test_single_model(model_path, images_path, coco_gt, category_mapping):\n",
    "    \"\"\"Test a single YOLO model and return results in COCO format\"\"\"\n",
    "    model_name = Path(model_path).stem\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Create image ID mapping\n",
    "    filename_to_id, id_to_filename = create_image_id_mapping(coco_gt, images_path)\n",
    "\n",
    "    # Get image list (only images that exist in ground truth)\n",
    "    image_files = [Path(images_path) / filename for filename in filename_to_id.keys()]\n",
    "\n",
    "    # Results storage\n",
    "    coco_results = []\n",
    "    timing_info = []\n",
    "\n",
    "    # Process images\n",
    "    for img_file in tqdm(image_files, desc=f\"Processing {model_name}\"):\n",
    "        img_filename = img_file.name\n",
    "        img_id = filename_to_id[img_filename]\n",
    "\n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        results = model(str(img_file), conf=CONFIG['confidence_threshold'],\n",
    "                       iou=CONFIG['iou_threshold'], max_det=CONFIG['max_detections'],\n",
    "                       verbose=False, save=False)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        timing_info.append({\n",
    "            'image_id': img_id,\n",
    "            'inference_time': inference_time,\n",
    "            'image_file': img_filename\n",
    "        })\n",
    "\n",
    "        # Convert to COCO format - CRITICAL FIX for bbox format\n",
    "        if results[0].boxes is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()  # x1,y1,x2,y2\n",
    "            scores = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                yolo_class = int(classes[i])\n",
    "\n",
    "                # Map YOLO class to COCO category\n",
    "                if yolo_class in category_mapping:\n",
    "                    coco_category = category_mapping[yolo_class]\n",
    "                else:\n",
    "                    print(f\"Warning: Unknown YOLO class {yolo_class}\")\n",
    "                    continue\n",
    "\n",
    "                # COCO bbox format: [x, y, width, height]\n",
    "                bbox = [float(x1), float(y1), float(x2-x1), float(y2-y1)]\n",
    "\n",
    "                # Validate bbox\n",
    "                if bbox[2] <= 0 or bbox[3] <= 0:\n",
    "                    continue\n",
    "\n",
    "                coco_results.append({\n",
    "                    'image_id': int(img_id),\n",
    "                    'category_id': int(coco_category),\n",
    "                    'bbox': bbox,\n",
    "                    'score': float(scores[i])\n",
    "                })\n",
    "\n",
    "    print(f\"  Generated {len(coco_results)} detections\")\n",
    "    return coco_results, timing_info, model_name\n",
    "\n",
    "def test_all_models():\n",
    "    \"\"\"Test all discovered models\"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            coco_results, timing_info, model_name = test_single_model(\n",
    "                model_path, CONFIG['images_path'], coco_gt, category_mapping\n",
    "            )\n",
    "\n",
    "            all_results[model_name] = {\n",
    "                'coco_results': coco_results,\n",
    "                'timing_info': timing_info,\n",
    "                'model_path': str(model_path)\n",
    "            }\n",
    "\n",
    "            # Save individual results\n",
    "            if CONFIG['save_coco_results']:\n",
    "                results_file = base_output_path / 'coco_results' / f'{model_name}_results.json'\n",
    "                with open(results_file, 'w') as f:\n",
    "                    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing model {model_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Test all models\n",
    "print(\"Starting model evaluation...\")\n",
    "model_results = test_all_models()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.894251Z",
     "iopub.execute_input": "2025-07-15T09:33:43.894501Z",
     "iopub.status.idle": "2025-07-15T09:33:43.915610Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.894480Z",
     "shell.execute_reply": "2025-07-15T09:33:43.914936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Metrics calculation functions ready\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metrics(coco_results, coco_gt, model_name):\n",
    "    \"\"\"Calculate comprehensive metrics for a model\"\"\"\n",
    "\n",
    "    if not coco_results:\n",
    "        return create_empty_metrics()\n",
    "\n",
    "    try:\n",
    "        # Filter results to only include images that exist in ground truth\n",
    "        valid_image_ids = set(coco_gt.getImgIds())\n",
    "        filtered_results = [r for r in coco_results if r['image_id'] in valid_image_ids]\n",
    "\n",
    "        if not filtered_results:\n",
    "            print(f\"No valid results found for {model_name} - no matching image IDs\")\n",
    "            return create_empty_metrics()\n",
    "\n",
    "        # Validate category IDs\n",
    "        valid_categories = set(coco_gt.getCatIds())\n",
    "        category_valid_results = [r for r in filtered_results if r['category_id'] in valid_categories]\n",
    "\n",
    "        if len(category_valid_results) != len(filtered_results):\n",
    "            print(f\"Warning: {len(filtered_results) - len(category_valid_results)} detections with invalid category IDs\")\n",
    "\n",
    "        filtered_results = category_valid_results\n",
    "\n",
    "        if not filtered_results:\n",
    "            print(f\"No valid results found for {model_name} after category validation\")\n",
    "            return create_empty_metrics()\n",
    "\n",
    "        print(f\"Evaluating {len(filtered_results)} valid detections for {model_name}\")\n",
    "\n",
    "        # Create temporary results file for COCO evaluation\n",
    "        temp_results_file = f'/tmp/{model_name}_temp_results.json'\n",
    "        with open(temp_results_file, 'w') as f:\n",
    "            json.dump(filtered_results, f)\n",
    "\n",
    "        # Load results with COCO API\n",
    "        coco_dt = coco_gt.loadRes(temp_results_file)\n",
    "\n",
    "        # COCO evaluation\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "\n",
    "        # Suppress print output from summarize\n",
    "        import sys\n",
    "        from io import StringIO\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "        coco_eval.summarize()\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "        # Extract metrics\n",
    "        metrics = {\n",
    "            'mAP_50_95': float(coco_eval.stats[0]) if coco_eval.stats[0] != -1 else 0.0,\n",
    "            'mAP_50': float(coco_eval.stats[1]) if coco_eval.stats[1] != -1 else 0.0,\n",
    "            'mAP_75': float(coco_eval.stats[2]) if coco_eval.stats[2] != -1 else 0.0,\n",
    "            'mAP_small': float(coco_eval.stats[3]) if coco_eval.stats[3] != -1 else 0.0,\n",
    "            'mAP_medium': float(coco_eval.stats[4]) if coco_eval.stats[4] != -1 else 0.0,\n",
    "            'mAP_large': float(coco_eval.stats[5]) if coco_eval.stats[5] != -1 else 0.0,\n",
    "            'AR_1': float(coco_eval.stats[6]) if coco_eval.stats[6] != -1 else 0.0,\n",
    "            'AR_10': float(coco_eval.stats[7]) if coco_eval.stats[7] != -1 else 0.0,\n",
    "            'AR_100': float(coco_eval.stats[8]) if coco_eval.stats[8] != -1 else 0.0,\n",
    "            'AR_small': float(coco_eval.stats[9]) if coco_eval.stats[9] != -1 else 0.0,\n",
    "            'AR_medium': float(coco_eval.stats[10]) if coco_eval.stats[10] != -1 else 0.0,\n",
    "            'AR_large': float(coco_eval.stats[11]) if coco_eval.stats[11] != -1 else 0.0,\n",
    "            'num_detections': len(coco_results),\n",
    "            'valid_detections': len(filtered_results)\n",
    "        }\n",
    "\n",
    "        # Clean up\n",
    "        os.remove(temp_results_file)\n",
    "\n",
    "        print(f\"COCO metrics for {model_name}: mAP@0.5={metrics['mAP_50']:.3f}, mAP@0.5:0.95={metrics['mAP_50_95']:.3f}\")\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating COCO metrics for {model_name}: {e}\")\n",
    "        return create_empty_metrics()\n",
    "\n",
    "def create_empty_metrics():\n",
    "    \"\"\"Create empty metrics dictionary\"\"\"\n",
    "    return {\n",
    "        'mAP_50': 0.0, 'mAP_75': 0.0, 'mAP_50_95': 0.0,\n",
    "        'mAP_small': 0.0, 'mAP_medium': 0.0, 'mAP_large': 0.0,\n",
    "        'AR_1': 0.0, 'AR_10': 0.0, 'AR_100': 0.0,\n",
    "        'AR_small': 0.0, 'AR_medium': 0.0, 'AR_large': 0.0,\n",
    "        'num_detections': 0, 'valid_detections': 0\n",
    "    }\n",
    "\n",
    "def calculate_timing_metrics(timing_info):\n",
    "    \"\"\"Calculate timing and performance metrics\"\"\"\n",
    "    if not timing_info:\n",
    "        return {'avg_inference_time': 0, 'fps': 0, 'total_time': 0}\n",
    "\n",
    "    times = [t['inference_time'] for t in timing_info]\n",
    "    return {\n",
    "        'avg_inference_time': np.mean(times),\n",
    "        'fps': 1.0 / np.mean(times),\n",
    "        'total_time': np.sum(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "        'std_time': np.std(times)\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all models\n",
    "all_metrics = {}\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"\\nCalculating metrics for {model_name}...\")\n",
    "\n",
    "    detection_metrics = calculate_metrics(\n",
    "        results['coco_results'], coco_gt, model_name\n",
    "    )\n",
    "    timing_metrics = calculate_timing_metrics(results['timing_info'])\n",
    "\n",
    "    all_metrics[model_name] = {\n",
    "        **detection_metrics,\n",
    "        **timing_metrics\n",
    "    }\n",
    "\n",
    "    # Save individual metrics\n",
    "    if CONFIG['save_metrics']:\n",
    "        metrics_file = base_output_path / 'metrics' / f'{model_name}_metrics.json'\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(all_metrics[model_name], f, indent=2)\n",
    "\n",
    "print(\"\\nMetrics calculation completed\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:33:43.916382Z",
     "iopub.execute_input": "2025-07-15T09:33:43.916637Z",
     "iopub.status.idle": "2025-07-15T09:43:34.330125Z",
     "shell.execute_reply.started": "2025-07-15T09:33:43.916613Z",
     "shell.execute_reply": "2025-07-15T09:43:34.329375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Starting evaluation on 6418 images...\n\nEvaluating model 1/6: yolo8n_pt_512_128_orig_full.pt\nCreating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                           \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Completed: 4928 detections, 0.217 precision, 0.097 recall\n\nEvaluating model 2/6: yolo8n_pt_512_64_sama_full.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                          \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Completed: 4913 detections, 0.219 precision, 0.098 recall\n\nEvaluating model 3/6: yolo8n_pt_512_16_sama_full.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                          \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Completed: 4848 detections, 0.216 precision, 0.095 recall\n\nEvaluating model 4/6: yolo8n_yaml_512_sama_full.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                         \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Completed: 4191 detections, 0.215 precision, 0.082 recall\n\nEvaluating model 5/6: yolo8n_pt_512_16_orig_full.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                          \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Completed: 4822 detections, 0.217 precision, 0.096 recall\n\nEvaluating model 6/6: yolo8n_yaml_512_orig_full.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                         \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Completed: 4150 detections, 0.217 precision, 0.082 recall\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def create_comprehensive_performance_plots():\n",
    "    \"\"\"Create comprehensive performance comparison visualizations\"\"\"\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    models = list(all_metrics.keys())\n",
    "    map_50 = [all_metrics[m]['mAP_50'] for m in models]\n",
    "    map_75 = [all_metrics[m]['mAP_75'] for m in models]\n",
    "    map_50_95 = [all_metrics[m]['mAP_50_95'] for m in models]\n",
    "    fps = [all_metrics[m]['fps'] for m in models]\n",
    "    avg_time = [all_metrics[m]['avg_inference_time'] for m in models]\n",
    "    num_detections = [all_metrics[m]['num_detections'] for m in models]\n",
    "\n",
    "    # Create comprehensive subplot layout\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 20))\n",
    "\n",
    "    # 1. mAP Comparison\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    axes[0, 0].bar(x - width, map_50, width, label='mAP@0.5', alpha=0.8)\n",
    "    axes[0, 0].bar(x, map_75, width, label='mAP@0.75', alpha=0.8)\n",
    "    axes[0, 0].bar(x + width, map_50_95, width, label='mAP@0.5:0.95', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Models')\n",
    "    axes[0, 0].set_ylabel('mAP Score')\n",
    "    axes[0, 0].set_title('mAP Comparison Across IoU Thresholds')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels([m.replace('_', '\\n') for m in models], rotation=0, fontsize=8)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (m50, m75, m50_95) in enumerate(zip(map_50, map_75, map_50_95)):\n",
    "        axes[0, 0].text(i - width, m50 + 0.01, f'{m50:.3f}', ha='center', fontsize=8)\n",
    "        axes[0, 0].text(i, m75 + 0.01, f'{m75:.3f}', ha='center', fontsize=8)\n",
    "        axes[0, 0].text(i + width, m50_95 + 0.01, f'{m50_95:.3f}', ha='center', fontsize=8)\n",
    "\n",
    "    # 2. FPS Comparison\n",
    "    bars = axes[0, 1].bar(models, fps, color='green', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Models')\n",
    "    axes[0, 1].set_ylabel('FPS')\n",
    "    axes[0, 1].set_title('Inference Speed (FPS)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, f in zip(bars, fps):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                       f'{f:.1f}', ha='center', va='bottom')\n",
    "\n",
    "    # 3. Speed vs Accuracy Trade-off\n",
    "    scatter = axes[1, 0].scatter(fps, map_50, s=200, alpha=0.7, c=map_50_95, cmap='viridis')\n",
    "    for i, model in enumerate(models):\n",
    "        axes[1, 0].annotate(model.replace('_', '\\n'), (fps[i], map_50[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[1, 0].set_xlabel('FPS')\n",
    "    axes[1, 0].set_ylabel('mAP@0.5')\n",
    "    axes[1, 0].set_title('Speed vs Accuracy Trade-off\\n(Color represents mAP@0.5:0.95)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='mAP@0.5:0.95')\n",
    "\n",
    "    # 4. Detection Count Comparison\n",
    "    bars = axes[1, 1].bar(models, num_detections, color='orange', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Models')\n",
    "    axes[1, 1].set_ylabel('Number of Detections')\n",
    "    axes[1, 1].set_title('Total Detections per Model')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, num_detections):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "                       f'{count}', ha='center', va='bottom')\n",
    "\n",
    "    # 5. Inference Time Distribution\n",
    "    axes[2, 0].bar(models, avg_time, color='purple', alpha=0.7)\n",
    "    axes[2, 0].set_xlabel('Models')\n",
    "    axes[2, 0].set_ylabel('Average Inference Time (s)')\n",
    "    axes[2, 0].set_title('Average Inference Time per Image')\n",
    "    axes[2, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Radar Chart for Multi-metric Comparison\n",
    "    if len(models) <= 6:  # Only if not too many models\n",
    "        categories = ['mAP@0.5', 'mAP@0.75', 'mAP@0.5:0.95', 'FPS\\n(normalized)', 'Precision\\n(mAP@0.5)']\n",
    "\n",
    "        # Normalize FPS for radar chart\n",
    "        max_fps = max(fps)\n",
    "        normalized_fps = [f/max_fps for f in fps]\n",
    "\n",
    "        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "\n",
    "        ax_radar = plt.subplot(3, 2, 6, projection='polar')\n",
    "\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            values = [map_50[i], map_75[i], map_50_95[i], normalized_fps[i], map_50[i]]\n",
    "            values += values[:1]  # Complete the circle\n",
    "\n",
    "            ax_radar.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "            ax_radar.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "\n",
    "        ax_radar.set_xticks(angles[:-1])\n",
    "        ax_radar.set_xticklabels(categories)\n",
    "        ax_radar.set_ylim(0, 1)\n",
    "        ax_radar.set_title('Multi-metric Model Comparison\\n(Radar Chart)')\n",
    "        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    else:\n",
    "        axes[2, 1].text(0.5, 0.5, 'Too many models\\nfor radar chart',\n",
    "                       transform=axes[2, 1].transAxes, ha='center', va='center', fontsize=12)\n",
    "        axes[2, 1].set_title('Radar Chart (Skipped)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base_output_path / 'visualizations' / 'comprehensive_performance.png',\n",
    "                dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_detailed_analysis_plots():\n",
    "    \"\"\"Create detailed analysis plots\"\"\"\n",
    "\n",
    "    # Efficiency Analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    models = list(all_metrics.keys())\n",
    "    map_50 = [all_metrics[m]['mAP_50'] for m in models]\n",
    "    fps = [all_metrics[m]['fps'] for m in models]\n",
    "\n",
    "    # 1. Efficiency Score (mAP * FPS)\n",
    "    efficiency = [m * f for m, f in zip(map_50, fps)]\n",
    "    bars = axes[0, 0].bar(models, efficiency, color='teal', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Models')\n",
    "    axes[0, 0].set_ylabel('Efficiency Score (mAP@0.5 Ã— FPS)')\n",
    "    axes[0, 0].set_title('Model Efficiency Analysis')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, eff in zip(bars, efficiency):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                       f'{eff:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    # 2. Precision vs Recall (using mAP as approximation)\n",
    "    ar_100 = [all_metrics[m]['AR_100'] for m in models]\n",
    "    axes[0, 1].scatter(ar_100, map_50, s=150, alpha=0.7, c=fps, cmap='plasma')\n",
    "    for i, model in enumerate(models):\n",
    "        axes[0, 1].annotate(model.replace('_', '\\n'), (ar_100[i], map_50[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Average Recall (AR@100)')\n",
    "    axes[0, 1].set_ylabel('mAP@0.5')\n",
    "    axes[0, 1].set_title('Precision vs Recall Analysis')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Model Ranking by different metrics\n",
    "    ranking_metrics = ['mAP_50', 'mAP_75', 'mAP_50_95', 'fps']\n",
    "    ranking_data = []\n",
    "\n",
    "    for metric in ranking_metrics:\n",
    "        values = [all_metrics[m][metric] for m in models]\n",
    "        ranks = np.argsort(np.argsort(values)[::-1]) + 1  # Ranking (1 is best)\n",
    "        ranking_data.append(ranks)\n",
    "\n",
    "    ranking_df = pd.DataFrame(ranking_data, index=ranking_metrics, columns=models)\n",
    "\n",
    "    im = axes[1, 0].imshow(ranking_df.values, cmap='RdYlGn_r', aspect='auto')\n",
    "    axes[1, 0].set_xticks(range(len(models)))\n",
    "    axes[1, 0].set_xticklabels([m.replace('_', '\\n') for m in models], rotation=45)\n",
    "    axes[1, 0].set_yticks(range(len(ranking_metrics)))\n",
    "    axes[1, 0].set_yticklabels(ranking_metrics)\n",
    "    axes[1, 0].set_title('Model Ranking Heatmap\\n(1=Best, Higher=Worse)')\n",
    "\n",
    "    # Add ranking numbers\n",
    "    for i in range(len(ranking_metrics)):\n",
    "        for j in range(len(models)):\n",
    "            axes[1, 0].text(j, i, f'{ranking_df.iloc[i, j]:.0f}',\n",
    "                           ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "    # 4. Performance Distribution\n",
    "    all_map_50 = [all_metrics[m]['mAP_50'] for m in models]\n",
    "    all_fps = [all_metrics[m]['fps'] for m in models]\n",
    "\n",
    "    axes[1, 1].hist(all_map_50, bins=5, alpha=0.7, label='mAP@0.5', color='blue')\n",
    "    axes[1, 1].set_xlabel('mAP@0.5')\n",
    "    axes[1, 1].set_ylabel('Number of Models')\n",
    "    axes[1, 1].set_title('mAP@0.5 Distribution')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add secondary y-axis for FPS\n",
    "    ax2 = axes[1, 1].twinx()\n",
    "    ax2.hist(all_fps, bins=5, alpha=0.7, label='FPS', color='red')\n",
    "    ax2.set_ylabel('Number of Models (FPS)', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base_output_path / 'visualizations' / 'detailed_analysis.png',\n",
    "                dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Generate enhanced visualizations\n",
    "print(\"Creating comprehensive visualizations...\")\n",
    "create_comprehensive_performance_plots()\n",
    "create_detailed_analysis_plots()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:43:34.332156Z",
     "iopub.execute_input": "2025-07-15T09:43:34.332548Z",
     "iopub.status.idle": "2025-07-15T09:43:34.338035Z",
     "shell.execute_reply.started": "2025-07-15T09:43:34.332531Z",
     "shell.execute_reply": "2025-07-15T09:43:34.337447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Results saved to evaluation_results/metrics.json\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "def get_ground_truth_for_image(coco_gt, img_id):\n",
    "    \"\"\"Get ground truth annotations for a specific image\"\"\"\n",
    "    ann_ids = coco_gt.getAnnIds(imgIds=[img_id])\n",
    "    anns = coco_gt.loadAnns(ann_ids)\n",
    "\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "\n",
    "    for ann in anns:\n",
    "        # COCO bbox format: [x, y, width, height]\n",
    "        x, y, w, h = ann['bbox']\n",
    "        gt_boxes.append([x, y, x+w, y+h])  # Convert to [x1, y1, x2, y2]\n",
    "\n",
    "        # Get category name\n",
    "        cat_info = coco_gt.loadCats([ann['category_id']])[0]\n",
    "        gt_labels.append(cat_info['name'])\n",
    "\n",
    "    return gt_boxes, gt_labels\n",
    "\n",
    "def create_sample_detections():\n",
    "    \"\"\"Create sample images with detections and ground truth\"\"\"\n",
    "\n",
    "    # Create image ID mapping for samples\n",
    "    filename_to_id, id_to_filename = create_image_id_mapping(coco_gt, CONFIG['images_path'])\n",
    "\n",
    "    # Get sample images\n",
    "    available_images = list(filename_to_id.keys())[:CONFIG['num_sample_images']]\n",
    "\n",
    "    for img_idx, img_filename in enumerate(available_images):\n",
    "        print(f\"Processing sample image {img_idx + 1}/{len(available_images)}: {img_filename}\")\n",
    "\n",
    "        img_file = Path(CONFIG['images_path']) / img_filename\n",
    "        img_id = filename_to_id[img_filename]\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_file))\n",
    "        if img is None:\n",
    "            print(f\"Could not load image: {img_file}\")\n",
    "            continue\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get ground truth\n",
    "        gt_boxes, gt_labels = get_ground_truth_for_image(coco_gt, img_id)\n",
    "\n",
    "        # Create subplot for GT + each model\n",
    "        num_models = len(model_files)\n",
    "        total_plots = num_models + 1  # +1 for ground truth\n",
    "        cols = min(3, total_plots)\n",
    "        rows = (total_plots + cols - 1) // cols\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "        if total_plots == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "\n",
    "        # Plot ground truth first\n",
    "        ax = axes[0]\n",
    "        ax.imshow(img_rgb)\n",
    "\n",
    "        for i, (gt_box, gt_label) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "            x1, y1, x2, y2 = gt_box\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                               fill=False, color='green', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1-5, gt_label,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"green\", alpha=0.7),\n",
    "                   fontsize=8, color='white')\n",
    "\n",
    "        ax.set_title(f'Ground Truth\\n({len(gt_boxes)} annotations)')\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Plot model predictions\n",
    "        for model_idx, model_path in enumerate(model_files):\n",
    "            model_name = Path(model_path).stem\n",
    "\n",
    "            try:\n",
    "                # Run inference\n",
    "                model = YOLO(model_path)\n",
    "                results = model(str(img_file), conf=CONFIG['confidence_threshold'],\n",
    "                               verbose=False, save=False)\n",
    "\n",
    "                # Plot predictions\n",
    "                ax = axes[model_idx + 1]\n",
    "                ax.imshow(img_rgb)\n",
    "\n",
    "                num_detections = 0\n",
    "                if results[0].boxes is not None:\n",
    "                    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                    scores = results[0].boxes.conf.cpu().numpy()\n",
    "                    classes = results[0].boxes.cls.cpu().numpy()\n",
    "                    num_detections = len(boxes)\n",
    "\n",
    "                    for i in range(len(boxes)):\n",
    "                        x1, y1, x2, y2 = boxes[i]\n",
    "                        score = scores[i]\n",
    "                        yolo_class = int(classes[i])\n",
    "\n",
    "                        # Get class name\n",
    "                        if yolo_class in category_mapping:\n",
    "                            coco_cat_id = category_mapping[yolo_class]\n",
    "                            cat_info = coco_gt.loadCats([coco_cat_id])[0]\n",
    "                            class_name = cat_info['name']\n",
    "                        else:\n",
    "                            class_name = f\"class_{yolo_class}\"\n",
    "\n",
    "                        # Draw bounding box\n",
    "                        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                           fill=False, color='red', linewidth=2)\n",
    "                        ax.add_patch(rect)\n",
    "\n",
    "                        # Add label\n",
    "                        ax.text(x1, y1-5, f'{class_name} {score:.2f}',\n",
    "                               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.7),\n",
    "                               fontsize=8, color='white')\n",
    "\n",
    "                ax.set_title(f'{model_name}\\n({num_detections} detections)')\n",
    "                ax.axis('off')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_name}: {e}\")\n",
    "                ax = axes[model_idx + 1]\n",
    "                ax.text(0.5, 0.5, f'Error: {model_name}',\n",
    "                       transform=ax.transAxes, ha='center', va='center')\n",
    "                ax.axis('off')\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for i in range(total_plots, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(base_output_path / 'samples' / f'sample_{img_idx + 1}_{img_filename}',\n",
    "                   dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"  Saved sample with {len(gt_boxes)} GT annotations\")\n",
    "\n",
    "    print(f\"Generated {len(available_images)} sample detection images\")\n",
    "\n",
    "# Generate sample detections\n",
    "print(\"\\nCreating sample detections with ground truth...\")\n",
    "create_sample_detections()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:43:34.338841Z",
     "iopub.execute_input": "2025-07-15T09:43:34.339073Z",
     "iopub.status.idle": "2025-07-15T09:43:37.070695Z",
     "shell.execute_reply.started": "2025-07-15T09:43:34.339059Z",
     "shell.execute_reply": "2025-07-15T09:43:37.069948Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_markdown_report():\n",
    "    \"\"\"Generate comprehensive and well-formatted markdown report\"\"\"\n",
    "\n",
    "    # Calculate additional statistics\n",
    "    total_gt_annotations = len(gt_data['annotations'])\n",
    "    total_images = len(gt_data['images'])\n",
    "    total_categories = len(gt_data['categories'])\n",
    "\n",
    "    # Get category statistics\n",
    "    category_stats = {}\n",
    "    for ann in gt_data['annotations']:\n",
    "        cat_id = ann['category_id']\n",
    "        if cat_id not in category_stats:\n",
    "            cat_name = next(cat['name'] for cat in gt_data['categories'] if cat['id'] == cat_id)\n",
    "            category_stats[cat_id] = {'name': cat_name, 'count': 0}\n",
    "        category_stats[cat_id]['count'] += 1\n",
    "\n",
    "    # Model performance rankings\n",
    "    models_by_map50 = sorted(all_metrics.items(), key=lambda x: x[1]['mAP_50'], reverse=True)\n",
    "    models_by_speed = sorted(all_metrics.items(), key=lambda x: x[1]['fps'], reverse=True)\n",
    "    models_by_efficiency = sorted(all_metrics.items(), key=lambda x: x[1]['mAP_50'] * x[1]['fps'], reverse=True)\n",
    "\n",
    "    report_content = f\"\"\"# ðŸŽ¯ YOLO Model Evaluation Report\n",
    "\n",
    "## ðŸ“‹ Experiment Overview\n",
    "\n",
    "| **Parameter** | **Value** |\n",
    "|---------------|-----------|\n",
    "| **Experiment Name** | `{CONFIG['experiment_name']}` |\n",
    "| **Date & Time** | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |\n",
    "| **Dataset** | Pascal VOC Test Dataset |\n",
    "| **Models Tested** | {len(model_results)} |\n",
    "| **Test Images** | {total_images} |\n",
    "| **Ground Truth Annotations** | {total_gt_annotations} |\n",
    "| **Object Categories** | {total_categories} |\n",
    "\n",
    "## âš™ï¸ Configuration Settings\n",
    "\n",
    "| **Setting** | **Value** |\n",
    "|-------------|-----------|\n",
    "| **Confidence Threshold** | {CONFIG['confidence_threshold']} |\n",
    "| **IoU Threshold** | {CONFIG['iou_threshold']} |\n",
    "| **Max Detections** | {CONFIG['max_detections']} |\n",
    "| **Image Size** | {CONFIG['image_size']} |\n",
    "| **Device** | {CONFIG['device']} |\n",
    "\n",
    "## ðŸ“Š Dataset Statistics\n",
    "\n",
    "### Category Distribution\n",
    "| **Category** | **Annotations** | **Percentage** |\n",
    "|--------------|-----------------|----------------|\n",
    "\"\"\"\n",
    "\n",
    "    # Add category statistics\n",
    "    for cat_id, info in sorted(category_stats.items(), key=lambda x: x[1]['count'], reverse=True):\n",
    "        percentage = (info['count'] / total_gt_annotations) * 100\n",
    "        report_content += f\"| {info['name']} | {info['count']} | {percentage:.1f}% |\\n\"\n",
    "\n",
    "    report_content += f\"\"\"\n",
    "## ðŸ† Model Performance Comparison\n",
    "\n",
    "### Overall Performance Table\n",
    "| **Model** | **mAP@0.5** | **mAP@0.75** | **mAP@0.5:0.95** | **FPS** | **Avg Time (s)** | **Detections** | **Efficiency** |\n",
    "|-----------|-------------|--------------|------------------|---------|------------------|----------------|-----------------|\n",
    "\"\"\"\n",
    "\n",
    "    # Add model performance rows\n",
    "    for model_name in sorted(all_metrics.keys()):\n",
    "        metrics = all_metrics[model_name]\n",
    "        efficiency = metrics['mAP_50'] * metrics['fps']\n",
    "        report_content += f\"| {model_name} | {metrics['mAP_50']:.3f} | {metrics['mAP_75']:.3f} | {metrics['mAP_50_95']:.3f} | {metrics['fps']:.1f} | {metrics['avg_inference_time']:.3f} | {metrics['num_detections']} | {efficiency:.2f} |\\n\"\n",
    "\n",
    "    # Performance rankings\n",
    "    report_content += f\"\"\"\n",
    "### ðŸ¥‡ Performance Rankings\n",
    "\n",
    "#### By Accuracy (mAP@0.5)\n",
    "\"\"\"\n",
    "    for i, (model_name, metrics) in enumerate(models_by_map50[:5]):\n",
    "        medal = \"ðŸ¥‡\" if i == 0 else \"ðŸ¥ˆ\" if i == 1 else \"ðŸ¥‰\" if i == 2 else f\"{i+1}.\"\n",
    "        report_content += f\"{medal} **{model_name}** - {metrics['mAP_50']:.3f}\\n\"\n",
    "\n",
    "    report_content += f\"\"\"\n",
    "#### By Speed (FPS)\n",
    "\"\"\"\n",
    "    for i, (model_name, metrics) in enumerate(models_by_speed[:5]):\n",
    "        medal = \"ðŸ¥‡\" if i == 0 else \"ðŸ¥ˆ\" if i == 1 else \"ðŸ¥‰\" if i == 2 else f\"{i+1}.\"\n",
    "        report_content += f\"{medal} **{model_name}** - {metrics['fps']:.1f} FPS\\n\"\n",
    "\n",
    "    report_content += f\"\"\"\n",
    "#### By Efficiency (mAP@0.5 Ã— FPS)\n",
    "\"\"\"\n",
    "    for i, (model_name, metrics) in enumerate(models_by_efficiency[:5]):\n",
    "        medal = \"ðŸ¥‡\" if i == 0 else \"ðŸ¥ˆ\" if i == 1 else \"ðŸ¥‰\" if i == 2 else f\"{i+1}.\"\n",
    "        efficiency = metrics['mAP_50'] * metrics['fps']\n",
    "        report_content += f\"{medal} **{model_name}** - {efficiency:.2f}\\n\"\n",
    "\n",
    "    # Detailed analysis\n",
    "    best_accuracy = max(all_metrics.items(), key=lambda x: x[1]['mAP_50'])\n",
    "    fastest_model = max(all_metrics.items(), key=lambda x: x[1]['fps'])\n",
    "    most_detections = max(all_metrics.items(), key=lambda x: x[1]['num_detections'])\n",
    "    best_efficiency = max(all_metrics.items(), key=lambda x: x[1]['mAP_50'] * x[1]['fps'])\n",
    "\n",
    "    report_content += f\"\"\"\n",
    "## ðŸ“ˆ Detailed Performance Analysis\n",
    "\n",
    "### Key Findings\n",
    "- **ðŸŽ¯ Accuracy Champion**: {best_accuracy[0]} achieved the highest mAP@0.5 of **{best_accuracy[1]['mAP_50']:.3f}**\n",
    "- **âš¡ Speed Champion**: {fastest_model[0]} processes images at **{fastest_model[1]['fps']:.1f} FPS**\n",
    "- **ðŸ” Detection Champion**: {most_detections[0]} generated **{most_detections[1]['num_detections']} detections**\n",
    "- **âš–ï¸ Efficiency Champion**: {best_efficiency[0]} with efficiency score of **{best_efficiency[1]['mAP_50'] * best_efficiency[1]['fps']:.2f}**\n",
    "\n",
    "### Model-Specific Analysis\n",
    "\"\"\"\n",
    "\n",
    "    # Add detailed analysis for each model\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        efficiency = metrics['mAP_50'] * metrics['fps']\n",
    "        report_content += f\"\"\"\n",
    "#### {model_name}\n",
    "- **Accuracy**: mAP@0.5 = {metrics['mAP_50']:.3f} | mAP@0.75 = {metrics['mAP_75']:.3f} | mAP@0.5:0.95 = {metrics['mAP_50_95']:.3f}\n",
    "- **Speed**: {metrics['fps']:.1f} FPS ({metrics['avg_inference_time']:.3f}s per image)\n",
    "- **Detections**: {metrics['num_detections']} total ({metrics['valid_detections']} valid)\n",
    "- **Efficiency Score**: {efficiency:.2f}\n",
    "\"\"\"\n",
    "\n",
    "    report_content += f\"\"\"\n",
    "## ðŸ“ Generated Files\n",
    "\n",
    "### ðŸ“Š Visualizations\n",
    "- `visualizations/comprehensive_performance.png` - Complete performance comparison\n",
    "- `visualizations/detailed_analysis.png` - In-depth analysis charts\n",
    "- `samples/` - Sample detection visualizations ({len(list((base_output_path / 'samples').glob('*.png')))} files)\n",
    "\n",
    "### ðŸ“„ Data Files\n",
    "- `coco_results/` - COCO format results ({len(list((base_output_path / 'coco_results').glob('*.json')))} files)\n",
    "- `metrics/` - Detailed metrics JSON files ({len(list((base_output_path / 'metrics').glob('*.json')))} files)\n",
    "\n",
    "## ðŸŽ¯ Recommendations\n",
    "\n",
    "### For Different Use Cases:\n",
    "\n",
    "#### ðŸŽ¯ High Accuracy Applications\n",
    "- **Recommended**: {best_accuracy[0]}\n",
    "- **Reason**: Highest mAP@0.5 of {best_accuracy[1]['mAP_50']:.3f}\n",
    "- **Use Cases**: Medical imaging, security systems, quality control\n",
    "\n",
    "#### âš¡ Real-Time Applications\n",
    "- **Recommended**: {fastest_model[0]}\n",
    "- **Reason**: Fastest processing at {fastest_model[1]['fps']:.1f} FPS\n",
    "- **Use Cases**: Video surveillance, autonomous vehicles, live streaming\n",
    "\n",
    "#### âš–ï¸ Balanced Performance\n",
    "- **Recommended**: {best_efficiency[0]}\n",
    "- **Reason**: Best efficiency score of {best_efficiency[1]['mAP_50'] * best_efficiency[1]['fps']:.2f}\n",
    "- **Use Cases**: General object detection, mobile applications\n",
    "\n",
    "### Performance Trade-offs\n",
    "- **Accuracy vs Speed**: There's a {(max(m['mAP_50'] for m in all_metrics.values()) - min(m['mAP_50'] for m in all_metrics.values())):.3f} difference in mAP@0.5 between best and worst models\n",
    "- **Speed Range**: FPS ranges from {min(m['fps'] for m in all_metrics.values()):.1f} to {max(m['fps'] for m in all_metrics.values()):.1f}\n",
    "- **Detection Volume**: Models generate between {min(m['num_detections'] for m in all_metrics.values())} and {max(m['num_detections'] for m in all_metrics.values())} detections\n",
    "\n",
    "## ðŸ“ Technical Notes\n",
    "\n",
    "### COCO Evaluation Metrics\n",
    "- **mAP@0.5**: Mean Average Precision at IoU threshold 0.5\n",
    "- **mAP@0.75**: Mean Average Precision at IoU threshold 0.75\n",
    "- **mAP@0.5:0.95**: Mean Average Precision averaged over IoU thresholds 0.5 to 0.95\n",
    "- **AR**: Average Recall at different detection limits\n",
    "\n",
    "### Validation Status\n",
    "- Image ID mapping: âœ… Validated\n",
    "- Category mapping: âœ… Validated\n",
    "- COCO format compliance: âœ… Validated\n",
    "- Detection format: âœ… Validated\n",
    "\n",
    "---\n",
    "*ðŸ“Š Report generated automatically by YOLO Model Evaluation System*\n",
    "*ðŸ• Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*ðŸ“§ For questions or issues, check the validation logs in the evaluation output*\n",
    "\"\"\"\n",
    "\n",
    "    # Save report\n",
    "    with open(base_output_path / 'evaluation_report.md', 'w') as f:\n",
    "        f.write(report_content)\n",
    "\n",
    "    print(\"ðŸ“„ Enhanced markdown report generated successfully\")\n",
    "    print(f\"ðŸ“ Location: {base_output_path / 'evaluation_report.md'}\")\n",
    "\n",
    "# Generate report\n",
    "generate_markdown_report()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:43:37.071633Z",
     "iopub.execute_input": "2025-07-15T09:43:37.071937Z",
     "iopub.status.idle": "2025-07-15T09:46:51.675295Z",
     "shell.execute_reply.started": "2025-07-15T09:43:37.071913Z",
     "shell.execute_reply": "2025-07-15T09:46:51.674507Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# ðŸ” DATA VALIDATION AND INTEGRITY CHECKING BLOCK\n",
    "# ============================================================================\n",
    "\n",
    "def validate_image_id_mapping():\n",
    "    \"\"\"Validate that image IDs match between YOLO results and COCO ground truth\"\"\"\n",
    "    print(\"ðŸ” Validating image ID mapping...\")\n",
    "\n",
    "    # Get all image IDs from ground truth\n",
    "    gt_image_ids = set(coco_gt.getImgIds())\n",
    "\n",
    "    # Get all image IDs from predictions\n",
    "    pred_image_ids = set()\n",
    "    for model_name, results in model_results.items():\n",
    "        for detection in results['coco_results']:\n",
    "            pred_image_ids.add(detection['image_id'])\n",
    "\n",
    "    # Check overlap\n",
    "    common_ids = gt_image_ids.intersection(pred_image_ids)\n",
    "    missing_in_pred = gt_image_ids - pred_image_ids\n",
    "    extra_in_pred = pred_image_ids - gt_image_ids\n",
    "\n",
    "    print(f\"  âœ… Ground truth images: {len(gt_image_ids)}\")\n",
    "    print(f\"  âœ… Prediction images: {len(pred_image_ids)}\")\n",
    "    print(f\"  âœ… Common images: {len(common_ids)}\")\n",
    "\n",
    "    if missing_in_pred:\n",
    "        print(f\"  âš ï¸  Missing in predictions: {len(missing_in_pred)} images\")\n",
    "        print(f\"      Example missing IDs: {list(missing_in_pred)[:5]}\")\n",
    "\n",
    "    if extra_in_pred:\n",
    "        print(f\"  âš ï¸  Extra in predictions: {len(extra_in_pred)} images\")\n",
    "        print(f\"      Example extra IDs: {list(extra_in_pred)[:5]}\")\n",
    "\n",
    "    # Validate image files exist\n",
    "    images_path = Path(CONFIG['images_path'])\n",
    "    existing_files = len(list(images_path.glob('*.jpg')) + list(images_path.glob('*.png')))\n",
    "    print(f\"  âœ… Image files on disk: {existing_files}\")\n",
    "\n",
    "    return {\n",
    "        'gt_image_ids': len(gt_image_ids),\n",
    "        'pred_image_ids': len(pred_image_ids),\n",
    "        'common_ids': len(common_ids),\n",
    "        'missing_in_pred': len(missing_in_pred),\n",
    "        'extra_in_pred': len(extra_in_pred),\n",
    "        'files_on_disk': existing_files\n",
    "    }\n",
    "\n",
    "def validate_category_mapping():\n",
    "    \"\"\"Validate category mapping between YOLO classes and COCO categories\"\"\"\n",
    "    print(\"ðŸ” Validating category mapping...\")\n",
    "\n",
    "    # Get all categories from ground truth\n",
    "    gt_categories = {cat['id']: cat['name'] for cat in gt_data['categories']}\n",
    "\n",
    "    # Get all categories from predictions\n",
    "    pred_categories = set()\n",
    "    for model_name, results in model_results.items():\n",
    "        for detection in results['coco_results']:\n",
    "            pred_categories.add(detection['category_id'])\n",
    "\n",
    "    # Check category validity\n",
    "    valid_categories = set(gt_categories.keys())\n",
    "    invalid_categories = pred_categories - valid_categories\n",
    "\n",
    "    print(f\"  âœ… Ground truth categories: {len(gt_categories)}\")\n",
    "    print(f\"  âœ… Categories in predictions: {len(pred_categories)}\")\n",
    "    print(f\"  âœ… Valid prediction categories: {len(pred_categories.intersection(valid_categories))}\")\n",
    "\n",
    "    if invalid_categories:\n",
    "        print(f\"  âŒ Invalid categories in predictions: {invalid_categories}\")\n",
    "\n",
    "    # Print category mapping\n",
    "    print(\"  ðŸ“‹ Category mapping:\")\n",
    "    for yolo_class, coco_cat in category_mapping.items():\n",
    "        cat_name = gt_categories.get(coco_cat, 'Unknown')\n",
    "        print(f\"      YOLO {yolo_class} -> COCO {coco_cat} ({cat_name})\")\n",
    "\n",
    "    return {\n",
    "        'gt_categories': len(gt_categories),\n",
    "        'pred_categories': len(pred_categories),\n",
    "        'valid_pred_categories': len(pred_categories.intersection(valid_categories)),\n",
    "        'invalid_categories': len(invalid_categories)\n",
    "    }\n",
    "\n",
    "def validate_detection_format():\n",
    "    \"\"\"Validate detection format and bbox coordinates\"\"\"\n",
    "    print(\"ðŸ” Validating detection format...\")\n",
    "\n",
    "    total_detections = 0\n",
    "    valid_detections = 0\n",
    "    invalid_bbox_count = 0\n",
    "    negative_coords_count = 0\n",
    "\n",
    "    for model_name, results in model_results.items():\n",
    "        model_detections = len(results['coco_results'])\n",
    "        model_valid = 0\n",
    "\n",
    "        for detection in results['coco_results']:\n",
    "            total_detections += 1\n",
    "\n",
    "            # Check required fields\n",
    "            required_fields = ['image_id', 'category_id', 'bbox', 'score']\n",
    "            if all(field in detection for field in required_fields):\n",
    "                # Check bbox format [x, y, width, height]\n",
    "                bbox = detection['bbox']\n",
    "                if (len(bbox) == 4 and\n",
    "                    bbox[2] > 0 and bbox[3] > 0 and  # positive width/height\n",
    "                    bbox[0] >= 0 and bbox[1] >= 0):  # non-negative coordinates\n",
    "                    valid_detections += 1\n",
    "                    model_valid += 1\n",
    "                else:\n",
    "                    invalid_bbox_count += 1\n",
    "                    if bbox[0] < 0 or bbox[1] < 0:\n",
    "                        negative_coords_count += 1\n",
    "\n",
    "        print(f\"  ðŸ“Š {model_name}: {model_valid}/{model_detections} valid detections\")\n",
    "\n",
    "    print(f\"  âœ… Total detections: {total_detections}\")\n",
    "    print(f\"  âœ… Valid detections: {valid_detections}\")\n",
    "    print(f\"  âŒ Invalid bbox format: {invalid_bbox_count}\")\n",
    "    print(f\"  âŒ Negative coordinates: {negative_coords_count}\")\n",
    "\n",
    "    return {\n",
    "        'total_detections': total_detections,\n",
    "        'valid_detections': valid_detections,\n",
    "        'invalid_bbox_count': invalid_bbox_count,\n",
    "        'negative_coords_count': negative_coords_count\n",
    "    }\n",
    "\n",
    "def validate_coco_format():\n",
    "    \"\"\"Validate COCO format compliance\"\"\"\n",
    "    print(\"ðŸ” Validating COCO format compliance...\")\n",
    "\n",
    "    # Check ground truth structure\n",
    "    required_gt_fields = ['images', 'annotations', 'categories']\n",
    "    missing_gt_fields = [field for field in required_gt_fields if field not in gt_data]\n",
    "\n",
    "    if missing_gt_fields:\n",
    "        print(f\"  âŒ Missing GT fields: {missing_gt_fields}\")\n",
    "    else:\n",
    "        print(f\"  âœ… Ground truth structure: Valid\")\n",
    "\n",
    "    # Check annotations structure\n",
    "    if gt_data['annotations']:\n",
    "        sample_ann = gt_data['annotations'][0]\n",
    "        required_ann_fields = ['id', 'image_id', 'category_id', 'bbox', 'area']\n",
    "        missing_ann_fields = [field for field in required_ann_fields if field not in sample_ann]\n",
    "\n",
    "        if missing_ann_fields:\n",
    "            print(f\"  âŒ Missing annotation fields: {missing_ann_fields}\")\n",
    "        else:\n",
    "            print(f\"  âœ… Annotation structure: Valid\")\n",
    "\n",
    "    # Check categories structure\n",
    "    if gt_data['categories']:\n",
    "        sample_cat = gt_data['categories'][0]\n",
    "        required_cat_fields = ['id', 'name', 'supercategory']\n",
    "        missing_cat_fields = [field for field in required_cat_fields if field not in sample_cat]\n",
    "\n",
    "        if missing_cat_fields:\n",
    "            print(f\"  âŒ Missing category fields: {missing_cat_fields}\")\n",
    "        else:\n",
    "            print(f\"  âœ… Category structure: Valid\")\n",
    "\n",
    "    return {\n",
    "        'gt_structure_valid': len(missing_gt_fields) == 0,\n",
    "        'annotation_structure_valid': len(missing_ann_fields) == 0 if gt_data['annotations'] else True,\n",
    "        'category_structure_valid': len(missing_cat_fields) == 0 if gt_data['categories'] else True\n",
    "    }\n",
    "\n",
    "def validate_model_outputs():\n",
    "    \"\"\"Validate model output consistency\"\"\"\n",
    "    print(\"ðŸ” Validating model output consistency...\")\n",
    "\n",
    "    for model_name, results in model_results.items():\n",
    "        print(f\"  ðŸ“Š {model_name}:\")\n",
    "\n",
    "        # Check timing info\n",
    "        timing_info = results['timing_info']\n",
    "        coco_results = results['coco_results']\n",
    "\n",
    "        # Count detections per image\n",
    "        detections_per_image = {}\n",
    "        for detection in coco_results:\n",
    "            img_id = detection['image_id']\n",
    "            detections_per_image[img_id] = detections_per_image.get(img_id, 0) + 1\n",
    "\n",
    "        # Statistics\n",
    "        if detections_per_image:\n",
    "            avg_detections = np.mean(list(detections_per_image.values()))\n",
    "            max_detections = max(detections_per_image.values())\n",
    "            min_detections = min(detections_per_image.values())\n",
    "\n",
    "            print(f\"      - Images processed: {len(timing_info)}\")\n",
    "            print(f\"      - Images with detections: {len(detections_per_image)}\")\n",
    "            print(f\"      - Avg detections/image: {avg_detections:.1f}\")\n",
    "            print(f\"      - Max detections/image: {max_detections}\")\n",
    "            print(f\"      - Min detections/image: {min_detections}\")\n",
    "\n",
    "            # Check for outliers\n",
    "            outlier_images = [img_id for img_id, count in detections_per_image.items()\n",
    "                            if count > CONFIG['max_detections'] * 0.8]\n",
    "            if outlier_images:\n",
    "                print(f\"      - âš ï¸  High detection count images: {len(outlier_images)}\")\n",
    "\n",
    "        # Timing validation\n",
    "        if timing_info:\n",
    "            times = [t['inference_time'] for t in timing_info]\n",
    "            avg_time = np.mean(times)\n",
    "            std_time = np.std(times)\n",
    "\n",
    "            print(f\"      - Avg inference time: {avg_time:.3f}s (Â±{std_time:.3f}s)\")\n",
    "\n",
    "            # Check for timing outliers\n",
    "            outlier_threshold = avg_time + 2 * std_time\n",
    "            outliers = [t for t in times if t > outlier_threshold]\n",
    "            if outliers:\n",
    "                print(f\"      - âš ï¸  Timing outliers: {len(outliers)} images\")\n",
    "\n",
    "def check_metrics_consistency():\n",
    "    \"\"\"Check consistency between calculated metrics\"\"\"\n",
    "    print(\"ðŸ” Checking metrics consistency...\")\n",
    "\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        print(f\"  ðŸ“Š {model_name}:\")\n",
    "\n",
    "        # Check mAP hierarchy (should be mAP@0.5 >= mAP@0.75 >= mAP@0.5:0.95)\n",
    "        map_50 = metrics['mAP_50']\n",
    "        map_75 = metrics['mAP_75']\n",
    "        map_50_95 = metrics['mAP_50_95']\n",
    "\n",
    "        hierarchy_ok = map_50 >= map_75 >= map_50_95\n",
    "        print(f\"      - mAP hierarchy: {'âœ…' if hierarchy_ok else 'âŒ'} ({map_50:.3f} >= {map_75:.3f} >= {map_50_95:.3f})\")\n",
    "\n",
    "        # Check detection counts\n",
    "        total_dets = metrics['num_detections']\n",
    "        valid_dets = metrics['valid_detections']\n",
    "\n",
    "        if total_dets >= valid_dets:\n",
    "            print(f\"      - Detection counts: âœ… ({valid_dets}/{total_dets} valid)\")\n",
    "        else:\n",
    "            print(f\"      - Detection counts: âŒ ({valid_dets}/{total_dets} - more valid than total!)\")\n",
    "\n",
    "        # Check timing consistency\n",
    "        avg_time = metrics['avg_inference_time']\n",
    "        fps = metrics['fps']\n",
    "        expected_fps = 1.0 / avg_time if avg_time > 0 else 0\n",
    "\n",
    "        fps_diff = abs(fps - expected_fps)\n",
    "        fps_ok = fps_diff < 0.1  # Allow small numerical differences\n",
    "\n",
    "        print(f\"      - FPS consistency: {'âœ…' if fps_ok else 'âŒ'} (calculated: {expected_fps:.1f}, reported: {fps:.1f})\")\n",
    "\n",
    "def generate_validation_report():\n",
    "    \"\"\"Generate validation summary report\"\"\"\n",
    "    print(\"ðŸ“„ Generating validation report...\")\n",
    "\n",
    "    # Run all validations\n",
    "    image_validation = validate_image_id_mapping()\n",
    "    category_validation = validate_category_mapping()\n",
    "    detection_validation = validate_detection_format()\n",
    "    coco_validation = validate_coco_format()\n",
    "\n",
    "    # Create validation report\n",
    "    validation_report = f\"\"\"# ðŸ” Validation Report\n",
    "\n",
    "## Image ID Mapping\n",
    "- Ground truth images: {image_validation['gt_image_ids']}\n",
    "- Prediction images: {image_validation['pred_image_ids']}\n",
    "- Common images: {image_validation['common_ids']}\n",
    "- Missing in predictions: {image_validation['missing_in_pred']}\n",
    "- Extra in predictions: {image_validation['extra_in_pred']}\n",
    "- Files on disk: {image_validation['files_on_disk']}\n",
    "\n",
    "## Category Mapping\n",
    "- Ground truth categories: {category_validation['gt_categories']}\n",
    "- Prediction categories: {category_validation['pred_categories']}\n",
    "- Valid prediction categories: {category_validation['valid_pred_categories']}\n",
    "- Invalid categories: {category_validation['invalid_categories']}\n",
    "\n",
    "## Detection Format\n",
    "- Total detections: {detection_validation['total_detections']}\n",
    "- Valid detections: {detection_validation['valid_detections']}\n",
    "- Invalid bbox format: {detection_validation['invalid_bbox_count']}\n",
    "- Negative coordinates: {detection_validation['negative_coords_count']}\n",
    "\n",
    "## COCO Format Compliance\n",
    "- Ground truth structure: {'âœ…' if coco_validation['gt_structure_valid'] else 'âŒ'}\n",
    "- Annotation structure: {'âœ…' if coco_validation['annotation_structure_valid'] else 'âŒ'}\n",
    "- Category structure: {'âœ…' if coco_validation['category_structure_valid'] else 'âŒ'}\n",
    "\n",
    "## Overall Status\n",
    "{'âœ… All validations passed' if all([\n",
    "    image_validation['missing_in_pred'] == 0,\n",
    "    category_validation['invalid_categories'] == 0,\n",
    "    detection_validation['invalid_bbox_count'] == 0,\n",
    "    coco_validation['gt_structure_valid'],\n",
    "    coco_validation['annotation_structure_valid'],\n",
    "    coco_validation['category_structure_valid']\n",
    "]) else 'âš ï¸ Some validations failed - check details above'}\n",
    "\"\"\"\n",
    "\n",
    "    # Save validation report\n",
    "    with open(base_output_path / 'validation_report.md', 'w') as f:\n",
    "        f.write(validation_report)\n",
    "\n",
    "    print(f\"ðŸ“„ Validation report saved to: {base_output_path / 'validation_report.md'}\")\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-15T09:46:51.676169Z",
     "iopub.execute_input": "2025-07-15T09:46:51.676389Z",
     "iopub.status.idle": "2025-07-15T09:46:51.708477Z",
     "shell.execute_reply.started": "2025-07-15T09:46:51.676372Z",
     "shell.execute_reply": "2025-07-15T09:46:51.707936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Report generated: evaluation_results/experiment_report.md\n\nâœ… Evaluation complete! Results saved in 'evaluation_results' directory\nðŸ“Š Evaluated 6 models\nðŸ“ Check the experiment_report.md for detailed results\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# ðŸš€ EXECUTE VALIDATION BLOCK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” STARTING COMPREHENSIVE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Execute all validation functions\n",
    "validate_image_id_mapping()\n",
    "print()\n",
    "validate_category_mapping()\n",
    "print()\n",
    "validate_detection_format()\n",
    "print()\n",
    "validate_coco_format()\n",
    "print()\n",
    "validate_model_outputs()\n",
    "print()\n",
    "check_metrics_consistency()\n",
    "print()\n",
    "generate_validation_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… VALIDATION COMPLETED\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "zip_filename = f\"{CONFIG['experiment_name']}.zip\"\n",
    "folder_to_zip = \"evaluation_results\"\n",
    "\n",
    "print(f\"ðŸ“¦ Creating archive: {zip_filename}\")\n",
    "\n",
    "!zip -r -q {zip_filename} {folder_to_zip}/\n",
    "\n",
    "zip_path = f'/kaggle/working/{zip_filename}'\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    file_size = os.path.getsize(zip_path) / (1024*1024)  # Ñƒ MB\n",
    "    print(f\"\\nâœ… Archive created successfully!\")\n",
    "    print(f\"ðŸ“ File: {zip_filename}\")\n",
    "    print(f\"ðŸ“Š Size: {file_size:.1f} MB\")\n",
    "    print(f\"ðŸ“ Path: {zip_path}\")\n",
    "\n",
    "    print(f\"\\nðŸ“‹ Archive contents:\")\n",
    "    !zipinfo {zip_filename} | head -20\n",
    "\n",
    "    display(HTML(f'''\n",
    "    <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "        <h3>ðŸ“¥ Download Ready</h3>\n",
    "        <a href=\"{zip_filename}\" download style=\"background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;\">\n",
    "            ðŸ“¥ Download {zip_filename} ({file_size:.1f} MB)\n",
    "        </a>\n",
    "    </div>\n",
    "    '''))\n",
    "else:\n",
    "    print(\"âŒ Error: Archive not created!\")\n",
    "    print(\"ðŸ“ Checking working directory contents:\")\n",
    "    !ls -la /kaggle/working/"
   ]
  }
 ]
}
